<!DOCTYPE html>
<html>
    <head>
        <title>Part1</title>
    </head>
    <body>
        <style>
                body {
                        color: white;
                        background-color: black;
                }
                span {
                        color: red;
                        background-color: blue;

                }
                h3 {
                        color: aqua;
                        background-color: coral;
                        text-align: left;
                }
                #The_Evolution_of_Layers_in_Enterprise_Applications {
                        color: darkgreen;
                }
        </style>


        <h2 style="color: darkblue; background-color: yellow;">PART 1</h2>
        <h2 style="color: yellow; background-color: blue;">The Narratives</h2>
        <h3 id="Layering">Chapter 1</h3>
        <h2>Layering</h2>
        <p>Layering is one of the most common techniques that software designers use to break apart a complicated software system. You see it in machine architectures, where layers descend from a programming language with operating system calls into device drivers and CPU instruction sets, and into logic gates inside chips. Networking has FTP layered on top of TCP, which is on top of IP, which is on top of ethernet.</p>
        <p>When thinking of a system in terms of layers, you imagine the principal subsystems in the software arranged in some form of layer cake, where each layer rests on a lower layer. In this scheme the higher layer uses various services defined by the lower layer, but the lower layer is unaware of the higher layer. Furthermore, each layer usually hides its lower layers from the layers above, so layer 4 uses the services of layer 3, which uses the services of layer 2, but layer 4 is unaware of layer 2. (Not all layering architectures are opaque like this, but most are—or rather most are mostly opaque.)</p>
        <p>Breaking down a system into layers has a number of important benefits.</p>
                <ul>
                        <li><span>You can understand a single layer as a coherent whole without knowing much about the other layers. You can understand how to build an FTP service on top of TCP without knowing the details of how ethernet works.</span></li>
                        <li>You can substitute layers with alternative implementations of the same basic services. An FTP service can run without change over ethernet, PPP, or whatever a cable company uses.</li>
                        <li>You minimize dependencies between layers. If the cable company changes its physical transmission system, providing they make IP work, we don’t have to alter our FTP service.</li>
                        <li>Layers make good places for standardization. TCP and IP are standards because they define how their layers should operate.</li>
                        <li>Once you have a layer built, you can use it for many higher-level services. Thus, TCP/IP is used by FTP, telnet, SSH, and HTTP. Otherwise, all of these higher-level protocols would have to write their own lower-level protocols.</li>
                </ul>
        <p>Layering is an important technique, but there are downsides.</p>
                <ul>
                        <li>Layers encapsulate some, but not all, things well. As a result you sometimes get cascading changes. The classic example of this in a layered enterprise application is adding a field that needs to display on the UI, must be in the database, and thus must be added to every layer in between.</li>
                        <li>Extra layers can harm performance. At every layer things typically need to be transformed from one representation to another. However, the encapsulation of an underlying function often gives you efficiency gains that more than compensate. A layer that controls transactions can be optimized and will then make everything faster.</li>
                </ul>
        <p>But the hardest part of a layered architecture is deciding what layers to have and what the responsibility of each layer should be.</p>
        <hr />
        <h3 id="The_Evolution_of_Layers_in_Enterprise_Applications">The Evolution of Layers in Enterprise Applications</h3>
        <p>h systems, I don’t sense that people thought much of layers in those days. You wrote a program that manipulated some form of files (ISAM, VSAM, etc.), and that was your application. No layers need apply. The notion of layers became more apparent in the ’90s with the rise of client– server systems. These were two-layer systems: The client held the user interface and other application code, and the server was usually a relational database. Common client tools were VB, Powerbuilder, and Delphi. These made it particularly easy to build data-intensive applications, as they had UI widgets that were aware of SQL. Thus you could build a screen by dragging controls onto a design area and then using property sheets to connect the controls to the database.</p>
        <p>If the application was all about the display and simple update of relational data, then these client–server systems worked very well. The problem came with domain logic: business rules, validations, calculations, and the like. Usually people would write these on the client, but this was awkward and usually done by embedding the logic directly into the UI screens. As the domain logic got more complex, this code became very difficult to work with. Furthermore, embedding logic in screens made it easy to duplicate code, which meant that simple changes resulted in hunting down similar code in many screens.</p>
        <p>An alternative was to put the domain logic in the database as stored procedures. However, stored procedures gave limited structuring mechanisms, which again led to awkward code. Also, many people liked relational databases because SQL was a standard that would allow them to change their database vendor. Despite the fact that few people actually did this, many liked having the option to change vendors without too high a porting cost. Because they are all proprietary, stored procedures removed that option.</p>
        <p>At the same time that client–server was gaining popularity, the object-oriented world was rising. The object community had an answer to the problem of domain logic: Move to a three-layer system. In this approach you have a presentation layer for your UI, a domain layer for your domain logic, and a data source. This way you could move all of that intricate domain logic out of the UI and put it into a layer where you could structure it properly with objects. Despite this, the object bandwagon made little headway. The truth was that many systems were simple, or at least started that way. And although the threelayer approach had many benefits, the tooling for client–server was compelling if your problem was simple. The client–server tools also were difficult, or even impossible, to use in a three-layer configuration.</p>
        <p>I think the seismic shock here was the rise of the Web. Suddenly people wanted to deploy client–server applications with a Web browser. However, if all your business logic was buried in a rich client, then all your business logic needed to be redone to have a Web interface. A well-designed three-layer system could just add a new presentation layer and be done with it. Furthermore, with Java we saw an unashamedly object-oriented language hit the mainstream. The tools that appeared to build Web pages were much less tied to SQL and thus more amenable to a third layer.</p>
        <p>When people discuss layering, there’s often some confusion over the terms layer and tier. Often the two are used as synonyms, but most people see tier as implying a physical separation. Client–server systems are often described as two-tier systems, and the separation is physical: The client is a desktop and the server is a server. I use layer to stress that you don’t have to run the layers on different machines. A distinct layer of domain logic often runs on either a desktop or the database server. In this situation you have two nodes but three distinct layers. With a local database I can run all three layers on a single laptop, but there will still be three distinct layers.</p>
        <h3 id="The_Three_Principal_Layers">The Three Principal Layers</h3>
        <p>For this book I’m centering my discussion around an architecture of three primary layers: presentation, domain, and data source. (I’m following the names used in [Brown et al.]). Table 1.1 summarizes these layers.</p>
        <p><b>Presentation</b> logic is about how to handle the interaction between the user and the software. This can be as simple as a command-line or text-based menu system, but these days it’s more likely to be a rich-client graphics UI or an HTML-based browser UI. (In this book I use rich client to mean a Windows/ Swing/fat-client UI, as opposed to an HTML browser.) The primary responsibilities of the presentation layer are to display information to the user and to interpret commands from the user into actions upon the domain and data source.</p>
        <p><b>Data source</b> logic is about communicating with other systems that carry out tasks on behalf of the application. These can be transaction monitors, other applications, messaging systems, and so forth. For most enterprise applications the biggest piece of data source logic is a database that is primarily responsible for storing persistent data.</p>
        <p>The remaining piece is the <b>domain logic</b>, also referred to as business logic. This is the work that this application needs to do for the domain you’re working with. It involves calculations based on inputs and stored data, validation of any data that comes in from the presentation, and figuring out exactly what data source logic to dispatch, depending on commands received from the presentation.</p>
        <p>Sometimes the layers are arranged so that the domain layer completely hides the data source from the presentation. More often, however, the presentation accesses the data store directly. While this is less pure, it tends to work better in practice. The presentation may interpret a command from the user, use the data source to pull the relevant data out of the database, and then let the domain logic manipulate that data before presenting it on the glass.</p>
        <p>A single application can often have multiple packages of each of these three subject areas. An application designed to be manipulated not only by end users through a rich-client interface but also through a command line would have two presentations: one for the rich-client interface and one for the command line. Multiple data source components may be present for different databases, Table 1.1 Three Principal Layers Layer Responsibilities Presentation Provision of services, display of information (e.g., in Windows or HTML, handling of user request (mouse clicks, keyboard hits), HTTP requests, command-line invocations, batch API) Domain Logic that is the real point of the system Data Source Communication with databases, messaging systems, transaction managers, other packages THE THREE PRINCIPAL LAYERS 21 but would be particularly common for communication with existing packages. Even the domain may be broken into distinct areas relatively separate from each other. Certain data source packages may only be used by certain domain packages. So far I’ve talked about a user. This naturally raises the question of what happens when there is no a human being driving the software. This could be something new and fashionable like a Web service or something mundane and useful like a batch process. In the latter case the user is the client program. At this point it becomes apparent that there is a lot of similarity between the presentation and data source layers in that they both are about connection to the outside world. This is the logic behind Alistair Cockburn’s Hexagonal Architecture pattern [wiki], which visualizes any system as a core surrounded by interfaces to external systems. In Hexagonal Architecture everything external is fundamentally an outside interface, and thus it’s a symmetrical view rather than my asymmetric layering scheme.</p>
        <p>I find this asymmetry useful, however, because I think there is a good distinction to be made between an interface that you provide as a service to others and your use of someone else’s service. Driving down to the core, this is the real distinction I make between presentation and data source. Presentation is an external interface for a service your system offers to someone else, whether it be a complex human or a simple remote program. Data source is the interface to things that are providing a service to you. I find it beneficial to think about these differently ecause the difference in clients alters the way you think about the service. Although we can identify the three common responsibility layers of presentation, domain, and data source for every enterprise application, how you separate them depends on how complex the application is. A simple script to pull data from a database and display it in a Web page may all be one procedure. I would still endeavor to separate the three layers, but in that case I might do it only by placing the behavior of each layer in separate subroutines. As the system gets more complex, I would break the three layers into separate classes. As complexity increased I would divide the classes into separate packages. My general advice is to choose the most appropriate form of separation for your problem but make sure you do some kind of separation—at least at the subroutine level. Together with the separation, there’s also a steady rule about dependencies: The domain and data source should never be dependent on the presentation.</p>
        <p>That is, there should be no subroutine call from the domain or data source code into the presentation code. This rule makes it easier to substitute different presentations on the same foundation and makes it easier to modify the presentation without serious ramifications deeper down. The relationship between the domain and the data source is more complex and depends upon the architectural patterns used for the data source. One of the hardest parts of working with domain logic seems to be that people often find it difficult to recognize what is domain logic and what is other forms of logic. An informal test I like is to imagine adding a radically different layer to an application, such as a command-line interface to a Web application.</p>
        <p>If there’s any functionality you have to duplicate in order to do this, that’s a sign of where domain logic has leaked into the presentation. Similarly, do you have to duplicate logic to replace a relational database with an XML file? A good example of this is a system I was told about that contained a list of products in which all the products that sold over 10 percent more than they did the previous month were colored in red. To do this the developers placed logic in the presentation layer that compared this month’s sales to last month’s sales and if the difference was more than 10 percent, they set the color to red.</p>
        <p>The trouble is that that’s putting domain logic into the presentation. To properly separate the layers you need a method in the domain layer to indicate if a product has improving sales. This method does the comparison between the two months and returns a Boolean value. The presentation layer then simply calls this Boolean method and, if true, highlights the product in red. That way the process is broken into its two parts: deciding whether there is something highlightable and choosing how to highlight.</p>
        <p>I’m uneasy with being overly dogmatic about this. When reviewing this book, Alan Knight commented that he was “torn between whether just putting that into the UI is the first step on a slippery slope to hell or a perfectly reasonable thing to do that only a dogmatic purist would object to.” The reason we are uneasy is because it’s both!</p>
        <hr />
        <h3 id="Choosing_Where_to_Run_Your_Layers">Choosing Where to Run Your Layers</h3>
        <p>For most of this book I will be talking about logical layers—that is, dividing a system into separate pieces to reduce the coupling between different parts of a system. Separation between layers is useful even if the layers are all running on one physical machine. However, there are places where the physical structure of a system makes a difference.</p>
        <p>For most IS applications the decision is whether to run processing on a client, on a desktop machine, or on a server. Often the simplest case is to run everything on servers. An HTML front end that uses a Web browser is a good way to do this. The great advantage of run- ning on the server is that everything is easy to upgrade and fix because it’s in a limited amount of places. You don’t have to worry about deployment to many desktops and keeping them all in sync with the server. You don’t have to worry about compatibilities with other desktop software.</p>
        <p>The general argument in favor of running on a client turns on responsiveness or disconnected operation. Any logic that runs on the server needs a server roundtrip to respond to anything the user does. If the user wants to fiddle with things and see immediate feedback, that roundtrip gets in the way. It also needs a network connection to run. The network may like to be everywhere, but as I type this it isn’t at 31,000 feet. It may be everywhere soon, but there are people who want to do work now without waiting for wireless coverage to reach Dead End Creek. Disconnected operation brings particular challenges, and I’m afraid I decided to put those out of the scope of this book.</p>
        <p>With those general forces in place, we can look at the options layer by layer. The data source pretty much always runs only on servers. The exception is where you might duplicate server functionality onto a suitably powerful client, usually when you want disconnected operation. In this case changes to the data source on the disconnected client need to be synchronized with the server. As I mentioned earlier, I decided to leave those issues to another day—or another author.</p>
        <p>The decision of where to run the presentation depends mostly on what kind of user interface you want. Running a rich client pretty much means running the presentation on the client. Running a Web interface pretty much means running on the server. There are exceptions—for one, remote operation of client software (such as X servers in the Unix world) running a Web server on the desktop—but these exceptions are rare.</p>
        <p>If you’re building a B2C system, you have no choice. Any Tom, Dick, or Harriet can be connecting to your servers and you don’t want to turn anyone away because they insist on doing their online shopping with a TRS-80. In this case you do all processing on the server and offer up HTML for the browser to deal with. Your limitation with the HTML option is that every bit of decision making needs a roundtrip from the client to the server, and that can hurt responsiveness. You can reduce some of the lag with browser scripting and downloadable applets, but they reduce your browser compatibility and tend to add other headaches. The more pure HTML you can go, the easier life is. That ease of life is appealing even if every one of your desktops is lovingly hand-built by your IS department. Keeping clients up to date and avoiding compatibility errors with other software are problems even simple rich-client systems have.</p>
        <p>The primary reason that people want a rich-client presentation is that some tasks are complicated for users to do and, to have a usable application, they’ll need more than what a Web GUI can give. Increasingly, however, people are getting used to ways to make Web front ends more usable, and that reduces the need for a rich client presentation. As I write this I’m very much in favor of the Web presentation if you can and the rich client if you must. This leaves us with the domain logic. You can run business logic all on the server or all on the client, or you can split it. Again, all on the server is the best choice for ease of maintenance. The demand to move it to the client is for either responsiveness or disconnected use.</p>
        <p>If you have to run some logic on the client, you can consider running all of it there—at least that way it’s all in one place. Usually this goes hand in hand with a rich client—running a Web server on a client machine isn’t going to help responsiveness much, although it can be a way to deal with disconnected operation. In this case you can still keep your domain logic in separate modules from the presentation, with either a <i>Transaction Script</i> (110) or a <i>Domain Model</i> (116). The problem with putting all the domain logic on the client is that you have more to upgrade and maintain.</p>
        <p>Splitting across both the desktop and the server sounds like the worst of both worlds because you don’t know where any piece of logic may be. The main reason to do it is that you have only a small amount of domain logic that needs to run on the client. The trick then is to isolate this piece of logic in a self-contained module that isn’t dependent on any other part of the system. That way you can run that module on the client or the server. This will require a good bit of annoying jiggery-pokery, but it’s a good way of doing the job. Once you’ve chosen your processing nodes, you should try to keep all of a node’s code in a single process, either on one node or copied on several nodes in a cluster. Don’t try to separate the layers into discrete processes unless you absolutely have to. Doing that will both degrade performance and add complexity, as you have to add things like <i>Remote Facades</i> (388) and <i>Data Transfer Objects</i> (401).</p>
        <p>It’s important to remember that many of these things are what Jens Coldewey refers to as <b>complexity boosters</b>—distribution, explicit multithreading, paradigm chasms (such as object/relational), multiplatform development, and extreme performance requirements (such as more than 100 transactions per second). All of these carry a high cost. Certainly there are times when you have to do it, but never forget that each one carries a charge both in development and in on-going maintenance.25</p>
        <h3 id="Organizing_Domain_Logic">Chapter 2</h3>
        <h2>Organizing Domain Logic</h2>
        <p>In organizing domain logic I’ve separated it into three primary patterns: <i>Transaction Script</i> (110), Domain Model (116), and Table Module (125). The simplest approach to storing domain logic is the <i>Transaction Script</i> (110). A <i>Transaction Script</i> (110) is essentially a procedure that takes the input from the presentation, processes it with validations and calculations, stores data in the database, and invokes any operations from other systems. It then replies with more data to the presentation, perhaps doing more calculation to help organize and format the reply. The fundamental organization is of a single procedure for each action that a user might want to do. Hence, we can think of this pattern as being a script for an action, or business transaction. It doesn’t have to be a single inline procedure of code. Pieces get separated into subroutines, and these subroutines can be shared between different <i>Transaction Scripts</i> (110). However, the driving force is still that of a procedure for each action, so a retailing system might have Transaction Scripts (110) for checkout, for adding something to the shopping cart, for displaying delivery status, and so on.</p>
        <p>A <i>Transaction Script</i> (110) offers several advantages:
                <ul>
                        <li>It’s a simple procedural model that most developers understand.</li>
                        <li>It works well with a simple data source layer using <i>Row Data Gateway</i> (152) or <i>Table Data Gateway (144)</i>.</li>
                        <li>It’s obvious how to set the transaction boundaries: Start with opening a transaction and end with closing it. It’s easy for tools to do this behind the scenes.</li>
                </ul>
        </p>
        <p>Sadly, there are also plenty of disadvantages, which tend to appear as the complexity of the domain logic increases. Often there will be duplicated code as several transactions need to do similar things. Some of this can be dealt with by factoring out common subroutines, but even so much of the duplication is tricky to remove and harder to spot. The resulting application can end up being quite a tangled web of routines without a clear structure. Of course, complex logic is where objects come in, and the object-oriented way to handle this problem is with a <i>Domain Model</i> (116). With a <i>Domain Model</i> (116) we build a model of our domain which, at least on a first approximation, is organized primarily around the nouns in the domain. Thus, a leasing system would have classes for lease, asset, and so forth. The logic for handling validations and calculations would be placed into this domain model, so shipment object might contain the logic to calculate the shipping charge for a delivery. There might still be routines for calculating a bill, but such a procedure would quickly delegate to a <i>Domain Model</i> (116) method.</p>
        <p>Using a <i>Domain Model</i> (116) as opposed to a <i>Transaction Script</i> (110) is the essence of the paradigm shift that object-oriented people talk about so much. Rather than one routine having all the logic for a user action, each object takes a part of the logic that’s relevant to it. If you’re not used to a <i>Domain Model</i> (116), learning to work with one can be very frustrating as you rush from object to object trying to find where the behavior is. It’s hard to capture the essence of the difference between the two patterns with a simple example, but in the discussions of the patterns I’ve tried to do that by building a simple piece of domain logic both ways. The easiest way to see the difference is to look at sequence diagrams for the two approaches (Figures 2.1 and 2.2). The essential problem is that different kinds of product have different algorithms for recognizing revenue on a given contract (see Chapter 9, page 109, for more background). The calculation method has to determine what kind of product a given contract is for, apply the correct algorithm, and a Recognition Service calculateRecognitions (contractID) a Data Gateway a contract result set findContract (contractID) get data * insert revenue recognition Figure 2.1 A <i>Transaction Script’s</i> (110) way of calculating revenue recognitions. then create revenue recognition objects to capture the results of the calculation. (For simplicity I’m ignoring the database interaction issues.) In Figure 2.1, <i>Transaction Script’s</i> (110) method does all the work. The underlying objects are just <i>Table Data Gateways</i> (144), and all they do is pass data to the transaction script.</p>
        <p>In contrast, Figure 2.2 shows multiple objects, each forwarding part of the behavior to another until a strategy object creates the results. The value of a <i>Domain Model</i> (116) lies in the fact that once you’ve gotten used to things, there are many techniques that allow you to handle increasingly complex logic in a well-organized way. As we get more and more algorithms for calculating revenue recognition, we can add these by adding new recognition strategy objects. With <i>Transaction Script</i> (110) we’re adding more conditions to the conditional logic of the script. Once your mind is as warped to objects as mine is, you’ll find you prefer a <i>Domain Model</i> (116) even in fairly simple cases. The costs of a <i>Domain Model</i> (116) come from the complexity of using it and the complexity of your data source layer. It takes time for people new to rich object models to get used to a rich <i>Domain Model</i> (116). Often developers may need to spend several months working on a project that uses this pattern before their paradigms are shifted. However, when you’re used to <i>Domain Model</i> (116) you’re usually infected for life and it becomes easy to work with in the future—that’s how object bigots like me are made. However, a significant minority of developers seem to be unable to make the shift.</p>
        <p>Even once you’ve made the shift, you still have to deal with the database mapping. The richer your Domain Model (116), the more complex your mapping to a Contract calculateRecognitions a Product calculateRecognitions (a Contract) a Recognition Strategy calculateRecognitions (a Contract) a Revenue Recognition * new Figure 2.2 A <i>Domain Model’s</i> (116) way of calculating revenue recognitions. a relational database (usually with <i>Data Mapper</i> (165)). A sophisticated data source layer is much like a fixed cost—it takes a fair amount of money (if you uy) or time (if you build) to get a good one, but once you have it you can do a lot with it.</p>
        <p>There’s a third choice for structuring domain logic, Table Module (125). At very first blush the <i>Table Module</i> (125) looks like a <i>Domain Model</i> (116) since both have classes for contracts, products, and revenue recognitions. The vital difference is that a <i>Domain Model</i> (116) has one instance of contract for each contract in the database whereas a <i>Table Module</i> (125) has only one instance. A <i>Table Module</i> (125) is designed to work with a <i>Record Set</i> (508). Thus, the client of a contract Table Module (125) will first issue queries to the database to form a <i>Record Set</i> (508) and will create a contract object and pass it the Record Set (508) as an argument. The client can then invoke operations on the contract to do various things (Figure 2.3). If it wants to do something to an individual contract, it must pass in an ID.</p>
        <p>A <i>Table Module</i> (125) is in many ways a middle ground between a <i>Transaction Script</i> (110) and a Domain Model (116). Organizing the domain logic around tables rather than straight procedures provides more structure and makes it easier to find and remove duplication. However, you can’t use a number of the techniques that a Domain Model (116) uses for finer grained structure of the logic, such as inheritance, strategies, and other OO patterns. Figure 2.3 Calculating revenue recognitions with a Table Module (125). a Contract new (theDataSet) calculateRecognitions (contractID) a Product new (theDataSet) a Revenue Recognition new (theDataSet) getProductType (productID) * insert The biggest advantage of a Table Module (125) is how it fits into the rest of the architecture. Many GUI environments are built to work on the results of a SQL query organized in a Record Set (508). Since a Table Module (125) also works on a Record Set (508), you can easily run a query, manipulate the results in the Table Module (125), and pass the manipulated data to the GUI for display. You can also use the Table Module (125) on the way back for further validations and calculations. A number of platforms, particularly Microsoft’s COM and .NET, use this style of development.</p>
        <h3 id="Making_a_Choice">Making a Choice</h3>
        <p>So, how do you choose between the three patterns? It’s not an easy choice, and it very much depends on how complex your domain logic is. Figure 2.4 is one of those nonscientific graphs that really irritate me in PowerPoint presentations because they have utterly unquantified axes. However, it helps to visualize my sense of how the three compare. With simple domain logic the Domain Model (116) is less attractive because the cost of understanding it and the complexity of Effort to Enhance Complexity of Domain Logic Domain Model Table Module Transaction Script Figure 2.4 A sense of the relationships between complexity and effort for different domain logic styles. 30 ORGANIZING DOMAIN LOGIC the data source add a lot of effort to developing it that won’t be paid back. Nevertheless, as the complexity of the domain logic increases, the other approaches tend to hit a wall where adding more features becomes exponentially more difficult. Your problem, of course, is to figure out where on that x axis your application lies. The good news is that I can say that you should use a Domain Model (116) whenever the complexity of your domain logic is greater than 7.42. The bad news is that nobody knows how to measure the complexity of domain logic. In practice, then, all you can do is find some experienced people who can do an initial analysis of the requirements and make a judgment call. There are some factors that alter the curves a bit. A team that’s familiar with <i>Domain Model</i> (116) will lower the initial cost of using this pattern. It won’t lower it to same starting point as the others because of the data source complexity. Still, the better the team is, the more I’m inclined to use a <i>Domain Model</i> (116).</p>
        <p>The attractiveness of a <i>Table Module</i> (125) depends very much on the support for a common Record Set (508) structure in your environment. If you have an environment like .NET or Visual Studio, where lots of tools work around a <i>Record Set</i> (508), then that makes a <i>Table Module</i> (125) much more attractive. Indeed, I don’t see a reason to use <i>Transaction Scripts</i> (110) in a .NET environment. However, if there’s no special tooling for Record Sets (508), I wouldn’t bother with <i>Table Module</i> (125).</p>
        <p>Once you’ve made it, your decision isn’t completely cast in stone, but it is more tricky to change. So it’s worth some upfront thought to decide which way to go. If you find you went the wrong way, then, if you started with <i>Transaction Script</i> (110), don’t hesitate to refactor toward Domain Model (116). If you started with <i>Domain Model</i> (116), however, going to <i>Transaction Script</i> (110) is usually less worthwhile unless you can simplify your data source layer.</p>
        <p>These three patterns are not mutually exclusive choices. Indeed, it’s quite common to use <i>Transaction Script</i> (110) for some of the domain logic and <i>Table Module</i> (125) or <i>Domain Model</i> (116) for the rest.</p>
        <h3 id="Service_Layer">Service Layer</h3>
        <p>A common approach in handling domain logic is to split the domain layer in t\wo. A <i>Service Layer</i> (133) is placed over an underlying <i>Domain Model</i> (116) or <i>Table Module</i> (125). Usually you only get this with a <i>Domain Model</i> (116) or <i>Table Module</i> (125) since a domain layer that uses only <i>Transaction Script</i> (110) isn’t complex enough to warrant a separate layer. The presentation logic interacts with the domain purely through the <i>Service Layer</i> (133), which acts as an API for the application.</p>
        <p>As well as providing a clear API, the <i>Service Layer</i> (133) is also a good spot to place such things as transaction control and security. This gives you a simple model of taking each method in the <i>Service Layer</i> (133) and describing its transactional and security characteristics. A separate properties file is a common choice for this, but .NET’s attributes provide a nice way of doing it directly in the code.</p>
        <p>When you see a <i>Service Layer</i> (133), a key decision is how much behavior to put in it. The minimal case is to make the <i>Service Layer</i> (133) a facade so that all of the real behavior is in underlying objects and all the <i>Service Layer</i> (133) does is forward calls on the facade to lower-level objects. In that case the <i>Service Layer</i> (133) provides an API that’s easier to use because it’s typically oriented around use cases. It also makes a convenient point for adding transactional wrappers and security checks.</p>
        <p>At the other extreme, most business logic is placed in <i>Transaction Scripts</i> (110) inside the <i>Service Layer</i> (133). The underlying domain objects are very simple; if it’s a <i>Domain Model</i> (116) it will be one-to-one with the database and you can thus use a simpler data source layer such as <i>Active Record</i> (160).</p>
        <p>Midway between these alternatives is a more even mix of behavior: the controller-entity style. This name comes from a common practice influenced heavily by [Jacobson et al.]. The point here is to have logic that’s particular to a single transaction or use case placed in <i>Transaction Scripts</i> (110), which are commonly referred to as controllers or services. These are different controllers to the input controller in <i>Model View Controller</i> (330) or <i>Application Controller</i> (379) that we’ll meet later, so I use the term use-case controller. Behavior that’s used in more than one use case goes on the domain objects, which are called entities. Although the controller-entity approach is a common one, it’s not one that I’ve ever liked much. The use case controllers, like any <i>Transaction Script</i> (110), tend to encourage duplicate code. My view is that, if you decide to use a <i>Domain Model</i> (116) at all, you really should go whole hog and make it dominant. The one exception to this is if you’ve started with a design that uses <i>Transaction Script</i> (110) with <i>Row Data Gateway</i> (152). Then it makes sense to move duplicated behavior to the <i>Row Data Gateways</i> (152), which will turn them into a simple <i>Domain Model</i> (116) using <i>Active Record</i> (160). However, I wouldn’t start that way. I would only do that to improve a design that’s showing cracks.</p>
        <p>I’m saying not that you should never have service objects that contain business logic, but that you shouldn’t necessarily make a fixed layer of them. Procedural service objects can sometimes be a very useful way to factor logic, but I tend to use them as needed rather than as an architectural layer. My preference is thus to have the thinnest <p>Service Layer</p> (133) you can, if you even need one. My usual approach is to assume that I don’t need one and only add it if it seems that the application needs it. However, I know many good designers who always use a <i>Service Layer</i> (133) with a fair bit of logic, so feel free to ignore me on this one. Randy Stafford has had a lot of success with a rich <i>Service Layer</i> (133), which is why I asked him to write the <i>Service Layer</i> (133) pattern for this book. 33</p>
        <h3 id="Mapping_to_Relational_Databases">Chapter 3</h3>
        <h2>Mapping to Relational Databases</h2>
        <p>The role of the data source layer is to communicate with the various pieces of infrastructure that an application needs to do its job. A dominant part of this problem is talking to a database, which, for the majority of systems built today, means a relational database. Certainly there’s still a lot of data in older data storage formats, such as mainframe ISAM and VSAM files, but most people building systems today worry about working with a relational database.</p>
        <p>One of the biggest reasons for the success of relational databases is the presence of SQL, a mostly standard language for database communication. lthough SQL is full of annoying and complicated vendor-specific enhancements, its core syntax is common and well understood.</p>
        <h3 id="Architectural_Patterns">Architectural Patterns</h3>
        <p>The first set of patterns comprises the architectural patterns, which drive the way in which the domain logic talks to the database. The choice you make here is far-reaching for your design and thus difficult to refactor, so it’s one that you should pay some attention to. It’s also a choice that’s strongly affected by how you design your domain logic. Despite SQL’s widespread use in enterprise software, there are still pitfalls in using it. Many application developers don’t understand SQL well and, as a result, have problems defining effective queries and commands. Although various techniques exist for embedding SQL in a programming language, they’re all somewhat awkward. It would be better to access data using mechanisms that fit in with the application development language. Database administrators (DBAs) also like to get at the SQL that accesses a table so that they can understand how best to tune it and how to arrange indexes. 34 MAPPING TO RELATIONAL DATABASES For these reasons, it’s wise to separate SQL access from the domain logic and place it in separate classes. A good way of organizing these classes is to base them on the table structure of the database so that you have one class per database table. These classes then form a Gateway (466) to the table. The rest of the application needs to know nothing about SQL, and all the SQL that accesses the database is easy to find. Developers who specialize in the database have a clear place to go.</p>
        <p>There are two main ways in which you can use a Gateway (466). The most obvious is to have an instance of it for each row that’s returned by a query (Figure 3.1). This Row Data Gateway (152) is an approach that naturally fits an object-oriented way of thinking about the data. Many environments provide a Record Set (508)—that is, a generic data structure of tables and rows that mimics the tabular nature of a database. Because a Record Set (508) is a generic data structure, environments can use it in many parts of an application. It’s quite common for GUI tools to have controls that work with a Record Set (508). If you use a Record Set (508), you only need a single class for each table in the database. This Table Data Gateway (144) (see Figure 3.2) provides methods to query the database that return a ecord Set (508).</p>
        <pre>
        insert update
           delete find (id)
        findForCompany(companyID)
        lastname
        firstname
        numberOfDependents
        Person Gateway
        Figure 3.1 A Row Data Gateway (152) has one instance per row returned by a query.
        find (id) : RecordSet
        findWithLastName(String) : RecordSet
        update (id, lastname, firstname, numberOfDependents)
        insert (lastname, firstname, numberOfDependents)
        delete (id)
        </pre>
        <p>Person Gateway Figure 3.2 A Table Data Gateway (144) has one instance per table. ARCHITECTURAL PATTERNS 35 ven for simple applications I tend to use one of the gateway patterns. A glance at my Ruby and Python scripts will confirm this. I find the clear separation of SQL and domain logic to be very helpful. The fact that Table Data Gateway (144) fits very nicely with Record Set (508) makes it the obvious choice if you are using Table Module (125). It’s also a pattern you can use to think about organizing stored procedures. Many designers like to do all of their database access through stored procedures rather than through explicit SQL. In this case you can think of the collection of stored procedures as defining a Table Data Gateway (144) for a table. I would still have an in-memory Table Data Gateway (144) to wrap the calls to the stored procedures, since that keeps the mechanics of the stored procedure call encapsulated. If you’re using Domain Model (116), some further options come into play. Certainly you can use a Row Data Gateway (152) or a Table Data Gateway (144) with a Domain Model (116). For my taste, however, that can be either too much indirection or not enough. In simple applications the Domain Model (116) is an uncomplicated structure that actually corresponds pretty closely to the database structure, with one domain class per database table. Such domain objects often have only moderately complex business logic. In this case it makes sense to have each domain object be responsible for loading and saving from the database, which is Active Record (160) (see Figure 3.3). Another way to think of the Active Record (160) is that you start with a Row Data Gateway (152) and then add domain logic to the class, particularly when you see repetitive code in multiple Transaction Scripts (110). In this kind of situation the added indirection of a Gateway (466) doesn’t provide a great deal of value. As the domain logic gets more complicated and you begin moving toward a rich Domain Model (116), the simple approach of an Active Record (160) starts to break down. The one-to-one match of domain load(ResultSet) delete insert update checkCredit sendBills Customer Customer Table Figure 3.3 In the Active Record (160) a customer domain object knows how to interact with database tables. 36 MAPPING TO RELATIONAL DATABASES classes to tables starts to fail as you factor domain logic into smaller classes. Relational databases don’t handle inheritance, so it becomes difficult to use strategies [Gang of Four] and other neat OO patterns. As the domain logic gets feisty, you want to be able to test it without having to talk to the database all the time. All of these forces push you to in’direction as your Domain Model (116) gets richer. In this case the Gateway (466) can solve some problems, but it still leaves you with the Domain Model (116) coupled to the schema of the database. As a result there’s some transformation from the fields of the Gateway (466) to the fields of the domain objects, and this transformation complicates your domain objects. A better route is to isolate the Domain Model (116) from the database completely, by making your indirection layer entirely responsible for the mapping between domain objects and database tables. This Data Mapper (165) (see Figure 3.4) handles all of the loading and storing between the database and the Domain Model (116) and allows both to vary independently. It’s the most complicated of the database mapping architectures, but its benefit is complete isolation of the two layers. I don’t recommend using a Gateway (466) as the primary persistence mechanism for a Domain Model (116). If the domain logic is simple and you have a close correspondence between classes and tables, Active Record (160) is the simple way to go. If you have something more complicated, Data Mapper (165) is what you need. These patterns aren’t entirely mutually exclusive. In much of this discussion we’re thinking of the primary persistence mechanism, by which we mean how you save the data in some kind of in-memory model to the database. For that you’ll pick one of these patterns; you don’t want to mix them because that ends up getting very messy. Even if you’re using Data Mapper (165) as your primary persistence mechanism, however, you may use a data Gateway (466) to wrap tables or services that are being treated as external interfaces. create load save Customer Mapper Customer Table checkCredit sendBills Customer Figure 3.4 A Data Mapper (165) insulates the domain objects and the database from each other. ARCHITECTURAL PATTERNS 37 In my discussion of these ideas, both here and in the patterns themselves, I tend to use the word “table.” However, most of these techniques can apply equally well to views, queries encapsulated through stored procedures, and commonly used dynamic queries. Sadly, there isn’t a widely used term for table/ view/query/stored procedure, so I use “table” because it represents a tabular data structure. I usually think of views as virtual tables, which is of course how SQL thinks of them too. The same syntax is used for querying views as for querying tables. Updating obviously is more complicated with views and queries, as you can’t always update a view directly but instead have to manipulate the tables that underlie it. In this case encapsulating the view/query with an appropriate pattern is a very good way to implement that update logic in one place, which makes using the views both simpler and more reliable.</p>
        <p>One of the problems with using views and queries in this way is that it can lead to inconsistencies that may surprise developers who don’t understand how a view is formed. They may perform updates on two different structures, both of which update the same underlying tables where the second update overwrites an update made by the first. Providing that the update logic does proper validation, you shouldn’t get inconsistent data this way, but you may surprise your developers. I should also mention the simplest way of persisting even the most complex Domain Model (116). During the early days of objects many people realized that there was a fundamental “impedance mismatch” between objects and relations. Thus, there followed a spate of effort on object-oriented databases, which essentially brought the OO paradigm to disk storage. With an OO database you don’t have to worry about mapping. You work with a large structure of interconnected objects, and the database figures out when to move objects on or off disks. Also, you can use transactions to group together updates and permit sharing of the data store. To programmers this seems like an infinite amount of transactional memory that’s transparently backed by disk storage. The chief advantage of OO databases is that they improve productivity. Although I’m not aware of any controlled tests, anecdotal observations put the effort of mapping to a relational database at around a third of programming effort—a cost that continues during maintenance. Most projects don’t use OO databases, however. The primary reason against them is risk. Relational databases are a well-understood and proven technology backed by big vendors who have been around a long time. SQL provides a relatively standard interface for all sorts of tools. (If you’re concerned about performance, all I can say is that I haven’t seen any conclusive data comparing the performance of OO against that of relational systems.)</p>
        <p>Even if you can’t use an OO database, you should seriously consider buying an O/R mapping tool if you have a Domain Model (116). While the patterns in this book will tell you a lot about how to build a Data Mapper (165), it’s still a complicated endeavor. Tool vendors have spent many years working on this problem, and commercial O/R mapping tools are much more sophisticated than anything that can reasonably be done by hand. While the tools aren’t cheap, you have to compare their price with the considerable cost of writing and maintaining such a layer yourself. There are moves to provide an OO-database-style layer that can work with relational databases. JDO is such a beast in the Java world, but it’s still too early to tell how they’ll work out. I haven’t had enough experience with them to draw any conclusions for this book. Even if you do buy a tool, however, it’s a good idea to be aware of these patterns. Good O/R tools give you a lot of options in mapping to a database, and these patterns will help you understand when to use the different choices. Don’t assume that a tool makes all the effort go away. It makes a big dent, but you’ll still find that using and tuning an O/R tool takes a small but significant chunk  work.</p>
        <h3 id="The_Behavioral_Problem">The Behavioral Problem</h3>
        <p>When people talk about O/R mapping, they usually focus on the structural aspects—how you relate tables to objects. However, I’ve found that the hardest part of the exercise is its architectural and behavioral aspects. I’ve already talked about the main architectural approaches; the next thing to think about is the behavioral problem.</p>
        <p>That behavioral problem is how to get the various objects to load and save themselves to the database. At first sight this doesn’t seem to be much of a problem. A customer object can have load and save methods that do this task. Indeed, with <i>Active Record</i> (160) this is an obvious route to take. If you load a bunch of objects into memory and modify them, you have to keep track of which ones you’ve modified and make sure to write all of them back out to the database. If you only load a couple of records, this is easy. As you load more and more objects it gets to be more of an exercise, particularly when you create some rows and modify others since you’ll need the keys from the created rows before you can modify the rows that refer to them. This is a slightly tricky problem to solve.</p>
        <p>As you read objects and modify them, you have to ensure that the database state you’re working with stays consistent. If you read some objects, it’s important to ensure that the reading is isolated so that no other process changes any of the objects you’ve read while you’re working on them. Otherwise, you could have inconsistent and invalid data in your objects. This is the issue of concurrency, which is a very tricky problem to solve; we’ll talk about this in Chapter 5. A pattern that’s essential to solving both of these problems is <i>Unit of Work</i> (184). A <i>Unit of Work</i> (184) keeps track of all objects read from the database, together with all objects modified in any way. It also handles how updates are made to the database. Instead of the application programmer invoking explicit save methods, the programmer tells the unit of work to commit. That unit of work then sequences all of the appropriate behavior to the database, putting all of the complex commit processing in one place. <i>Unit of Work</i> (184) is an essential pattern whenever the behavioral interactions with the database become awkward. A good way of thinking about <i>Unit of Work</i> (184) is as an object that acts as the controller of the database mapping. Without a <i>Unit of Work</i> (184), typically the domain layer acts as the controller; deciding when to read and write to the database. The <i>Unit of Work</i> (184) results from factoring the database mapping controller behavior into its own object. As you load objects, you have to be wary about loading the same one twice. If you do that, you’ll have two in-memory objects that correspond to a single database row. Update them both, and everything gets very confusing. To deal with this you need to keep a record of every row you read in an <i>Identity Map</i> (195). Each time you read in some data, you check the <i>Identity Map</i> (195) first to make sure that you don’t already have it. If the data is already loaded, you can return a second reference to it. That way any updates will be properly coordinated. As a benefit you may also be able to avoid a database call since the <i>Identity Map</i> (195) also doubles as a cache for the database. Don’t forget, however, that the primary purpose of an Identity Map (195) is to maintain correct identities, not to boost performance.</p>
        <p>If you’re using a <i>Domain Model</i> (116), you’ll usually arrange things so that linked objects are loaded together in such a way that a read for an order object loads its associated customer object. However, with many objects connected together any read of any object can pull an enormous object graph out of the database. To avoid such inefficiencies you need to reduce what you bring back yet still keep the door open to pull back more data if you need it later on. <i>Lazy Load</i> (200) relies on having a placeholder for a reference to an object. There are several variations on the theme, but all of them have the object reference modified so that, instead of pointing to the real object, it marks a placeholder. Only if you try to follow the link does the real object get pulled in from the database. Using <i>Lazy Load</i> (200) at suitable points, you can bring back just enough from the database with each call</h6>
        <h3 id="Reading_in_Data">Reading in Data</h3>
        <p>When reading in data I like to think of the methods as finders that wrap SQL select statements with a method-structured interface. Thus, you might have methods such as <pre>find(id)</pre> or <pre>findForCustomer(customer)</pre>. Clearly these methods can get pretty unwieldy if you have 23 different clauses in your select statements, but these are, thankfully, rare.</p>
        <p>Where you put the finder methods depends on the interfacing pattern used. If your database interaction classes are table based—that is, you have one instance of the class per table in the database—then you can combine the finder methods with the inserts and updates. If your interaction classes are row based—that is, you have one interaction class per row in the database—this doesn’t work.</p>
        <p>With row-based classes you can make the find operations static, but doing so will stop you from making the database operations substitutable. This means that you can’t swap out the database for testing purposes with <i>Service Stub</i> (504). To avoid this problem the best approach is to have separate finder objects. Each finder class has many methods that encapsulate a SQL query. When you execute the query, the finder object returns a collection of the appropriate row-based objects. One thing to watch for with finder methods is that they work on the database state, not the object state. If you issue a query against the database to find all people within a club, remember that any person objects you’ve added to the club in memory won’t get picked up by the query. As a result it’s usually wise to do queries at the beginning.</p>
        <p>When reading in data, performance issues can often loom large. This leads to a few rules of thumb. Try to pull back multiple rows at once. In particular, never do repeated queries on the same table to get multiple rows. It’s almost always better to pull back too much data than too little (although you have to be wary of locking too many rows with pessimistic concurrency control). Therefore, consider a situation where you need to get 50 people that you can identify by a primary key in your domain model, but you can only construct a query such that you get 200 people, from which you’ll do some further logic to isolate the 50 you need. It’s usually better to use one query that brings back unnecessary rows than to issue 50 individual queries.</p>
        <p>Another way to avoid going to the database more than once is to use joins so that you can pull multiple tables back with a single query. The resulting record set looks odd but can really speed things up. In this case you may have a <i>Gateway</i> (466) that has data from multiple joined tables, or a Data Mapper (165) that loads several domain objects with a single call. However, if you’re using joins, bear in mind that databases are optimized to handle up to three or four joins per query. Beyond that, performance suffers, although you can restore a good bit of this with cached views. Many optimizations are possible in the database. These things involve clustering commonly referenced data together, careful use of indexes, and the database’s ability to cache in memory. These are outside the scope of this book but inside the scope of a good DBA.</p>
        <p>In all cases you should profile your application with your specific database and data. General rules can guide your thinking, but your particular circumstances will always have their own variations. Database systems and application servers often have sophisticated caching schemes, and there’s no way I can predict what will happen for your application. For every rule of thumb I’ve used, I’ve heard of surprising exceptions, so set aside time to do performance profiling and tuning.</p>
        <h3 id="Structural_Mapping_Patterns">Structural Mapping Patterns</h3>
        <p>When people talk about object-relational mapping, mostly what they mean is these kinds of structural mapping patterns, which you use when mapping between in-memory objects and database tables. These patterns aren’t usually relevant for Table <i>Data Gateway</i> (144), but you may use a few of them if you use <i>Row Data Gateway</i> (152) or Active Record (160). You’ll probably need to use all of them for <i>Data Mapper</i> (165).</p>
        <h4 id="Mapping_Relationships">Mapping Relationships</h4>
        <p>The central issue here is the different way in which objects and relations handle links, which leads to two problems. First there’s a difference in representation. Objects handle links by storing references that are held by the runtime of either memory-managed environments or memory addresses. Relational databases handle links by forming a key into another table. Second, objects can easily use collections to handle multiple references from a single field, while normalization forces all relation links to be single valued. This leads to reversals of the data structure between objects and tables. An order object naturally has a collection of line item objects that don’t need any reference back to the order. However, the table structure is the other way around—the line item must include a foreign key reference to the order since the order can’t have a multivalued field.</p>
        <p>The way to handle the representation problem is to keep the relational identity of each object as an <i>Identity Field</i> (216) in the object, and to look up these values to map back and forth between the object references and the relational keys. It’s a tedious process but not that difficult once you understand the basic technique. When you read objects from the disk you use an <i>Identity Map</i> (195) as a lookup table from relational keys to objects. Each time you come across a foreign key in the table, you use <i>Foreign Key Mapping</i> (236) (see Figure 3.5) to wire up the appropriate inter-object reference. If you don’t have the key in the <i>Identity Map</i> (195), you need to either go to the database to get it or use a <i>Lazy Load</i> (200). Each time you save an object, you save it into the row with the right key. Any inter-object reference is replaced with the target object’s ID field.</p>
        <p>On this foundation the collection handling requires a more complex version of <i>Foreign Key Mapping</i> (236) (see Figure 3.6). If an object has a collection, you need to issue another query to find all the rows that link to the ID of the source object (or you can now avoid the query with <i>Lazy Load</i> (200)). Each object that comes back gets created and added to the collection. Saving the collection involves saving each object in it and making sure it has a foreign key to the source object. This gets messy, especially when you have to detect objects added or removed from the collection. This can get repetitive when you get the hang of it, which is why some form of metadata-based approach becomes an obvious move for larger systems (I’ll elaborate on that later). If the collection objects aren’t used outside the scope of the collection’s owner, you can use Dependent Mapping (262) to simplify the mapping.</p>
        <p>A different case comes up with a many-to-many relationship, which has a collection on both ends. An example is a person having many skills and each skill knowing the people who use it. Relational databases can’t handle this directly, so you use an Association Table Mapping (248) (see Figure 3.7) to create a new relational table just to handle the many-to-many association.</p>
        <p>When you’re working with collections, a common gotcha is to rely on the ordering within the collection. In OO languages it’s common to use ordered collections such as lists and arrays—indeed, it often makes testing easier. Nevertheless, it’s very difficult to maintain an arbitrarily ordered collection when saved to a relational database. For this reason it’s worth considering using unordered sets for storing collections. Another option is to decide on a sort order whenever you do a collection query, although that can be quite expensive. In some cases referential integrity can make updates more complex. Modern systems allow you to defer referential integrity checking to the end of the transaction. If you have this capability, there’s no reason not to use it. Otherwise, the database will check on every write. In this case you have to be careful to do your updates in the right order. How to do this is out of the scope of this book, but one technique is to do a topological sort of your updates. Another is to hardcode which tables get written in which order. This can sometimes reduce deadlock problems inside the database that cause transactions to roll back too often.</p>
        <p><i>Identity Field</i> (216) is used for inter-object references that turn into foreign keys, but not all object relationships need to be persisted that way. <i>Small Value Objects</i> (486), such as date ranges and money objects clearly shouldn’t be represented as their own table in the database. Instead, take all the fields of the <i>Value Object</i> (486) and embed them into the linked object as an <i>Embedded Value</i> (268). Since <i>Value Objects</i> (486) have value semantics, you can happily create them each time you get a read and you don’t need to bother with an <i>Identity Map</i> (195). Writing them out is also easy—just dereference the object and spit out its fields into the owning table.</p>
        <p>You can do this kind of thing on a larger scale by taking a whole cluster of bjects and saving them as a single column in a table as a <i>Serialized LOB</i> (272). LOB stands for “Large OBject,” which can be either binary (BLOB) or textual (CLOB—Character Large OBject). Serializing a clump of objects as an XML document is an obvious route to take for a hierarchic object structure. This way you can grab a whole bunch of small linked objects in a single read. Often databases perform poorly with small highly interconnected objects—where you spend a lot of time making many small database calls. Hierarchic structures such as org charts and bills of materials are where a Serialized LOB (272) can save a lot of database roundtrips.</p>
        <p>The downside is that SQL isn’t aware of what’s happening, so you can’t make portable queries against the data structure. Again, XML may come to the rescue here, allowing you to embed XPath query expressions within SQL calls, although the embedding is largely nonstandard at the moment. As a result <i>Serialized LOB</i> (272) is best used when you don’t want to query for the parts of the stored structure. Usually a <i>Serialized LOB</i> (272) is best for a relatively isolated group of objects that make part of an application. If you use it too much, it ends up turning your database into little more than a transactional file system.</p>
        <h3 id="Inheritance">Inheritance</h3>
        <p>In the above hierarchies I’m talking about compositional hierarchies, such as a parts tree, which relational systems traditionally do poorly. There’s another kind of hierarchy that causes relational headaches: a class hierarchy linked by inheritance. Since there’s no standard way to do inheritance in SQL, we again have a mapping to perform. For any inheritance structure there are basically three options. You can have one table for all the classes in the hierarchy: Single <i>Table Inheritance</i> (278) (see Figure 3.8); one table for each concrete class: Concrete <i>Table Inheritance</i> (293) (see Figure 3.9); or one table per class in the hierarchy: <i>Class Table Inheritance</i> (285) (see Figure 3.10).</p>
        <p>The trade-offs are all between duplication of data structure and speed of access. Class <i>Table Inheritance</i> (285) is the simplest relationship between the classes and the tables, but it needs multiple joins to load a single object, which usually reduces performance. Concrete <i>Table Inheritance</i> (293) avoids the joins, allowing you pull a single object from one table, but it’s brittle to changes. With any change to a superclass you have to remember to alter all the tables (and the mapping code). Altering the hierarchy itself can cause even bigger changes. Also, the lack of a superclass table can make key management awkward and get in the way of referential integrity, although it does reduce lock contention on the superclass table. In some databases Single Table Inheritance (278)’s biggest downside is wasted space, since each row has to have columns for all possible subtypes and this leads to empty columns. However, many databases do a very good job of compressing wasted table space. Another problem with <i>Single Table Inheritance</i> (278) is its size, making it a bottleneck for accesses. Its great advantage is that it puts all the stuff in one place, which makes modification easier and avoids joins. The three options aren’t mutually exclusive, and in one hierarchy you can mix patterns. For instance, you could have several classes pulled together with <i>Single Table Inheritance</i> (278) and use <i>Class Table Inheritance</i> (285) for a few unusual cases. Of course, mixing patterns adds complexity.</p>
        <p>There’s no clearcut winner here. You need to take into account your own circumstances and preferences, much as with all the rest of these patterns. My first choice tends to be <i>Single Table Inheritance</i> (278), as it’s easy to do and is resilient to many refactorings. I tend to use the other two as needed to help solve the nevitable issues with irrelevant and wasted columns. Often the best is to talk to the DBAs; they often have good advice as to the sort of access that makes the most sense for the database.</p>
        <p>All the examples just described, and in the patterns, use single inheritance. Although multiple inheritance is becoming less fashionable these days and most languages are increasingly avoiding it, the issue still appears in O/R mapping when you use interfaces, as in Java and .NET. The patterns here don’t go into this topic specifically, but essentially you cope with multiple inheritance using variations of the trio of inheritance patterns. Single Table Inheritance (278) puts all superclasses and interfaces into the one big table, <i>Class Table Inheritance</i> (285) makes a separate table for each interface and superclass, and <i>Concrete Table Inheritance</i> (293) includes all interfaces and superclasses in each concrete table.</p>
        <h4 id="Building_the_Mapping">Building the Mapping</h4>
        <p>When you map to a relational database, there are essentially three situations that you encounter:
                <ul>
                        <li>You choose the schema yourself.</li>
                        <li>You have to map to an existing schema, which can’t be changed.</li>
                        <li>You have to map to an existing schema, but changes to it are negotiable.</li>
                </ul>
        </p>
        <p>The simplest case is where you’re doing the schema yourself and have little to moderate complexity in your domain logic, resulting in a <i>Transaction Script</i> (110) or <i>Table Module</i> (125) design. In this case you can design the tables around the data using classic database design techniques. Use a <i>Row Data Gateway</i> (152) or <i>Table Data Gateway</i> (144) to pull the SQL away from the domain logic.</p>
        <p>If you’re using a <i>Domain Model</i> (116), you should beware of a design that looks like a database design. In this case build your <i>Domain Model</i> (116) without regard to the database so that you can best simplify the domain logic. Treat the database design as a way of persisting the objects’ data. <i>Data Mapper</i> (165) gives you the most flexibility here, but it’s more complex. If a database design isomorphic to the <i>Domain Model</i> (116) makes sense, you might consider an <i>Active Record</i> (160) instead.</p>
        <p>Although building the model first is a reasonable way of thinking about it, this advice only applies within short iterative cycles. Spending six months building a database-free Domain Model (116) and then deciding to persist it once you’re done is highly risky. The danger is that the resulting design will have crippling performance problems that take too much refactoring to fix. Instead, build up the database with each iteration, of no more than six weeks in length and preferably fewer. That way you’ll get rapid and continuous feedback about how your database interactions work in practice. Within any particular task you should think about the <i>Domain Model</i> (116) first, but integrate each piece of <i>Domain Model (116)</i> in the database as you go.</p>
        <p>When the schema’s already there, your choices are similar but the process is slightly different. With simple domain logic you build <i>Row Data Gateway</i> (152) or <i>Table Data Gateway</i> (144) classes that mimic the database, and layer domain logic on top of that. With more complex domain logic you’ll need a <i>Domain Model</i> (116), which is highly unlikely to match the database design. Therefore, gradually build up the <i>Domain Model</i> (116) and include <i>Data Mappers</i> (165) to persist the data to the existing database.</p>
        <h4 id="Double_Mapping">Double Mapping</h4>
        <p>Occasionally I run into situations where the same kind of data needs to be pulled from more than one source. There may be multiple databases that hold the same data but have small differences in the schema because of some copy and paste reuse. (In this situation the amount of annoyance is inversely proportional to the amount of the difference.) Another possibility is using different mechanisms, storing the data sometimes in a database and sometimes in messages. You may want to pull similar data from both XML messages, CICS transactions, and relational tables.</p>
        <p>The simplest option is to have multiple mapping layers, one for each data source. However, if data is very similar this can lead to a lot of duplication. In this situation you might consider a two-step mapping scheme. The first step converts data from the in-memory schema to a logical data store schema. The logical data store schema is designed to maximize the similarities in the data source formats. The second step maps from the logical data store schema to the actual physical data store schema. This second step contains the differences. The extra step only pays for itself when you have many commonalities, so you should use it when you have similar but annoyingly different physical data stores. Treat the mapping from the logical data store to the physical data store as a <i>Gateway</i> (466) and use any of the mapping techniques to map from the application logic to the logical data store.</p>
        <h3 id="Using_Metadata">Using Metadata</h3>
        <p>In this book most of my examples use handwritten code. With simple and repetitive mapping this can lead to code that’s simple and repetitive—and repetitive code is a sign of something wrong with the design. There’s much you can do by factoring out common behaviors with inheritance and delegation—good, honest OO practices—but there’s also a more sophisticated approach using <i>Metadata Mapping</i> (306). <i>Metadata Mapping</i> (306) is based on boiling down the mapping into a metadata file that details how columns in the database map to fields in objects. The point of this is that once you have the metadata you can avoid the repetitive code by using either code generation or reflective programming. Using metadata buys you a lot of expressiveness from a little metadata. One line of metadata can say something like
        <pre>&lt;field name = customer targetClass = "Customer", dbColumn = "custID", targetTable = "customers"
            lowerBound = "1" upperBound = "1" setter = "loadCustomer"/&gt;</pre></p>
        <p>From that you can define the read and write code, automatically generate ad hoc joins, do all of the SQL, enforce the multiplicity of the relationship, and even do fancy things like computing write orders under the presence of referential integrity. This is why commercial O/R mapping tools tend to use metadata.</p>
        <p>When you use <i>Metadata Mapping</i> (306) you have the necessary foundation to build queries in terms of in-memory objects. A <i>Query Object</i> (316) allows you to build your queries in terms of in-memory objects and data in such a way that developers don’t need to know either SQL or the details of the relational schema. The <i>Query Object</i> (316) can then use the <i>Metadata Mapping</i> (306) to translate expressions based on object fields into the appropriate SQL. Take this far enough and you can form a <i>Repository</i> (322) that largely hides the database from view. Any queries to the database can be made as <i>Query Objects</i> (316) against a <i>Repository</i> (322), and developers can’t tell whether the objects were retrieved from memory or from the database. <i>Repository</i> (322) works well with rich <i>Domain Model</i> (116) systems.</p>
        <p>Despite the many advantages of metadata, in this book I’ve focused on handwritten examples because I think they’re easier to understand first. Once you get the hang of the patterns and can handwrite them for your application, you’ll be able to figure out how to use metadata to make matters easier.</p>
        <h3 id="Database_Connections">Database Connections</h3>
        <p>Most database interfaces rely on some kind of database connection object to act as the link between application code and the database. Typically a connection must be opened before you can execute commands against the database. Indeed, usually you need an explicit connection to create and execute a command. The whole time you execute the command this same connection must be open. Queries return a Record Set (508). Some interfaces provide for disconnected <i>Record Sets</i> (508), which can be manipulated after the connection is closed. Other interfaces provide only connected Record Sets (508), implying that the connection must remain open while the <i>Record Set</i> (508) is manipulated. If you’re running inside a transaction, usually the transaction is bound to a particular connection and the connection must remain open while it is taking place.</p>
        <p>In many environments it’s expensive to create a connection, which makes it worthwhile to create a connection pool. In this situation developers request a connection from the pool and release it when they’re done, instead of creating and closing the connection. Most platforms these days give you pooling, so you’ll rarely have to do it yourself. If you do have to do it yourself, first check to see if pooling actually does help performance. Increasingly environments make it quicker to create a new connection so there’s no need to pool. Environments that give you pooling often put it behind an interface that looks like creating a new connection. That way you don’t know whether you’re getting a brand new connection or one allocated from a pool. That’s a good thing, as the choice to pool or not is properly encapsulated. Similarly, closing the connection may not actually close it but just return it to the pool for someone else to use. In this discussion I’ll use “open” and “close,” which you can substitute for “getting” from the pool and “releasing” back to the pool. Expensive to create or not, connections need management. Since they’re expensive resources to manage, they must be closed as soon as you’re done using them. Furthermore, if you’re using a transaction, usually you need to ensure that every command inside a particular transaction goes with the same connection.</p>
        <p>The most common advice is to get a connection explicitly, using a call to a pool or connection manager, and then supply it to each database command you want to make. Once you’re done with the connection, close it. This advice leads to a couple of issues: making sure you have the connection everywhere you need it and ensuring that you don’t forget to close it at the end. To ensure that you have a connection where you need it there are two choices. One is to pass the connection around as an explicit parameter. The problem with this is that the connection gets added to all sorts of method calls where its only purpose is to be passed to some other method five layers down the call stack. Of course, this is the situation to bring out <i>Registry</i> (480). Since you don’t want multiple threads using the same connection, you’ll want a thread-scoped <i>Registry</i> (480).</p>
        <p>If you’re half as forgetful as I am, explicit closing isn’t such a good idea. It’s just too easy to forget to do it when you should. You also can’t close the connection with every command because you may be running inside a transaction and the closing will usually cause the transaction to roll back. Like a connection, memory is a resource that needs to be freed up when you’re not using it. Modern environments these days provide automatic memory management and garbage collection, so one way to ensure that connections are closed is to use the garbage collector. In this approach either the connection itself or some object that refers to it closes the connection during garbage collection. The good thing about this is that it uses the same management scheme that’s used for memory and so it’s both convenient and familiar. The problem is that the close of the connection only happens when the garbage collector actually reclaims the memory, and this can be quite a bit later than when the connection lost its last reference. As a result unreferenced connections may sit around a while before they’re closed. Whether this is a problem or not depends ery much on your specific environment.</p>
        <p>On the whole I don’t like relying on garbage collection. Other schemes— even explicit closing—are better. Still, garbage collection makes a good backup in case the regular scheme fails. After all, it’s better to have the connections close eventually than to have them hanging around forever. Since connections are so tied to transactions, a good way to manage them is to tie them to a transaction. Open a connection when you begin a transaction, and close it when you commit or roll back. Have the transaction know what connection it’s using so you can ignore the connection completely and just deal with the transaction. Since the transaction’s completion has a visible effect, it’s easier to remember to commit it and to spot if you forget. A <i>Unit of Work</i> (184) makes a natural fit to manage both the transaction and the connection.</p>
        <p>If you do things outside of your transaction, such as reading immutable data, you use a fresh connection for each command. Pooling can deal with any issues in creating short-lived connections. If you’re using a disconnected <i>Record Set</i> (508), you can open a connection to put the data in the record set and close it while you manipulate the Record Set (508) data. Then, when you’re done with the data, you can open a new connection, and transaction, to write the data out. If you do this, you’ll need to worry about the data being changed while the <i>Record Set</i> (508) was being manipulated. This is a topic I’ll talk about with concurrency control. The specifics of connection management are very much a feature of your database interaction software, so the strategy you use is often dictated by your environment.</p>
        <h3 id="Some_Miscellaneous_Points">Some Miscellaneous Points</h3>
        <p>You’ll notice that some of the code examples use select statements in the form select * from while others use named columns. Using select * can have serious problems in some database drivers, which break if a new column is added or a column is reordered. Although more modern environments don’t suffer from this, it’s not wise to use select * if you’re using positional indices to get information from columns, as a column reorder will break code. It’s okay to use column name indices with a select *, and indeed column name indices are clearer to read; however, column name indices may be slower, although that probably won’t make much difference given the time for the SQL call. As usual, measure to be sure.</p>
        <p>If you do use column number indices, you need to make sure that the accesses to the result set are very close to the definition of the SQL statement so they don’t get out of sync if the columns are reordered. Consequently, if you’re using <i>Table Data Gateway</i> (144), you should use column name indices as the result set is used by every piece of code that runs a find operation on the gateway. As a result it’s usually worth having simple create/read/update/delete test cases for each database mapping structure you use. This will help catch cases when your SQL gets out of sync with your code. It’s always worth making the effort to use static SQL that can be precompiled, rather than dynamic SQL that has to be compiled each time. Most platforms give you a mechanism for precompiling SQL. A good rule of thumb is to avoid using string concatenation to put together SQL queries. Many environments give you the ability to batch multiple SQL queries into a single database call. I haven’t done that for these examples, but it’s certainly a tactic you should use in production code. How you do it varies with the platform.</p>
        <p>For connections in these examples, I just conjure them up with a call to a “DB” object, which is a <i>Registry</i> (480). How you get a connection will depend on your environment so you’ll substitute this with whatever you need to do. I haven’t involved transactions in any of the patterns other than those on concurrency. Again, you’ll need to mix in whatever your environment needs.</p>
        <h3 id="Further_Reading">Further Reading</h3>
        <p>Object-relational mapping is a fact of life for most people, so it’s no surprise that there’s been a lot written on the subject. The surprise is that there isn’t a single coherent, complete, and up-to-date book, which is why I’ve devoted so much of this one to this tricky yet interesting subject. The nice thing about database mapping is that there’s a lot of ideas out there to steal from. The most victimized intellectual banks are [Brown and hitenack], [Ambler], [Yoder], and [Keller and Coldewey]. I’d certainly urge you to have a good surf through this material to supplement the patterns in this book. <i>This page intentionally left blank</i></p>
        <h3 id="Web_Presentation">Chapter 4</h3>
        <h2>Web Presentation</h2>
        <p>One of the biggest changes to enterprise applications in the last few years has been the rise of Web-browser-based user interfaces. They bring with them a lot of advantages: no client software to install, a common UI approach, and easy universal access. Also, many environments make it easy to build a Web app. Preparing a Web app begins with the server software itself. Usually this has\ some form of configuration file that indicates which URLs are to be handled by which programs. Often a single Web server can handle many kinds of programs. These programs may be dynamic and can be added to a server by placing them in an appropriate directory. The Web server’s job is to interpret the URL of a request and hand over control to a Web server program. There are two main forms of structuring a program in a Web server: as a script or as a server page.</p>
        <p>The script form is a program, usually with functions or methods to handle the HTTP call. Examples include CGI scripts and Java servlets. The program text can do pretty much anything a program can do, and the script can be broken down into subroutines, and can create and use other services. It gets data from the Web page by examining the HTTP request object, which is a string. In some environments it does this by regular expression searching of the request string— Perl’s ease of doing this makes it a popular choice for CGI scripts. Other platforms, such as Java servlets, do this parsing for the programmer, which allows the programmer to access the information from the request through a keyword interface. This at least means less regular expressions to mess with. The output of the Web server is another string—the response—which the script can write to using the usual write stream operations in the language.</p>
        <p>Writing an HTML response through stream commands is uncomfortable for programmers, and nearly impossible for nonprogrammers, who would otherwise be comfortable preparing HTML pages. This has led to the idea of server pages, where the program is structured around the returning text page. You write the return page in HTML and insert into the HTML scriptlets of code to execute at certain points. Examples of this approach include PHP, ASP, and JSP. The server page approach works well when there’s little processing of the response, such as “Show me the details of album # 1234.” Things get a lot more messy when you have to make decisions based on the input, such as different display formats for classical and jazz albums. Because the script style works best for interpreting the request and the server page style works best for formatting a response, there’s the obvious option to use a script for request interpretation and a server page for response formatting. This separation is in fact an old idea that first surfaced in user interfaces with the pattern Model View Controller (330). Combine it with the essential notion that nonpresentation logic should be factored out and we have a very good fit for the concepts of this pattern.</p>
        <p><i>Model View Controller</i> (330) (see Figure 4.1) is a widely referenced pattern but one that’s often misunderstood. Indeed, before Web apps appeared on the scene, most presentations of <i>Model View Controller</i> (330) I sat through would get it wrong. A main reason for the confusion was the use of the word “controller.” Controller is used in a number of different contexts, and I’ve usually found it used in a different way to that described in <i>Model View Controller</i> (330). As a result I prefer to use the term input controller for the controller in <i>Model View Controller</i> (330).</p>
        <p>A request comes in to an input controller, which pulls information off the request. It then forwards the business logic to an appropriate model object. The model object talks to the data source and does everything indicated by the request as well as gather information for the response. When it’s done it returns control to the input controller, which looks at the results and decides which view is needed to display the response. It then passes control, together with the response data, to the view. The input controller’s handoff to the view often isn’t lways a straight call but often involves forwarding with the data placed in an agreed place on some form of HTTP session object that’s shared between the input controller and the view.</p>
        <p>The first, and most important, reason for applying <i>Model View Controller</i> (330) is to ensure that the models are completely separated from the Web presentation. This makes it easier to modify the presentation as well as easier to add additional presentations later. Putting the processing into separate <i>Transaction Script</i> (110) or Domain Model (116) objects will make it easier to test them as well.This is particularly important if you’re using a server page as your view. At this point we come to a second use of the word “controller.” A lot of userinterface designs separate the presentation objects from the domain objects with an intermediate layer of <i>Application Controller</i> (379) objects. The purpose of an <i>Application Controller</i> (379) is to handle the flow of an application, deciding which screens should appear in which order. It may appear as part of the presentation layer, or you can think of it as a separate layer that mediates between the presentation and domain layers. <i>Application Controllers</i> (379) may be written to be independent of any particular presentation, in which case they can be reused between presentations. This works well if you have different presentations with the same basic flow and navigation, although often it’s best to give different presentations a different flow.</p>
        <p>Not all systems need an <i>Application Controller</i> (379). They’re useful if your system has a lot of logic about the order of screens and the navigation between them. They’re also useful if you haven’t got a simple mapping between your pages and the actions on the domain. But if someone can pretty much see any screen in any order, you’ll probably have little need for an <i>Application Controller</i> (379). A good test is this: If the machine is in control of the screen flow, you need an <i>Application Controller</i> (379); if the user is in control, you don’t.</p>
        <h3 id="View_Patterns">View Patterns</h3>
        <p>On the view side there are three patterns to think about: <i>Transform View</i> (361), <i>Template View</i> (350), and <i>Two Step View</i> (365). These give rise to essentially two choices: whether to use <i>Transform View</i> (361) or <i>Template View</i> (350), and whether either of them uses one stage or a <i>Two Step View</i> (365). The basic patterns for <i>Transform View</i> (361) and <i>Template View</i> (350) are single stage. <i>Two Step View</i> (365) is a variation you can apply to either. I’ll start with the choice between <i>Template View</i> (350) and <i>Transform View</i> (361). <i>Template View</i> (350) allows you write the presentation in the structure of the page and embed markers into the page to indicate where dynamic content needs to go. Quite a few popular platforms are based on this pattern, many of which are the server pages technologies (ASP, JSP, PHP) that allow you to put a full programming language into the page. This clearly provides a lot of power and flexibility; sadly, it also leads to very messy code that’s difficult to maintain. As a result if you use server page technology you must be very disciplined to keep programming logic out of the page structure, often by using a helper object.</p>
        <p>The <i>Transform View</i> (361) uses a transform style of program. The usual example is XSLT. This can be very effective if you’re working with domain data that’s in XML format or can easily be converted to it. An input controller picks the appropriate XSLT stylesheet and applies it to XML gleaned from the model. If you use procedural scripts as your view, you can write the code in the style of either <i>Transform View</i> (361) or <i>Template View</i> (350) or in some interesting mix of the two. I’ve noticed that most scripts follow one of these two patterns as their main form.</p>
        <p>The second decision is whether to be single stage (see Figure 4.2) or to use Two Step View (365). A single-stage view mostly has one view component for each screen in the application. The view takes domain oriented data and renders it in HTML. I say “mostly” because similar logical screens may share views. Even so, most of the time you can think of it as one view per screen. A two-stage view (Figure 4.3) breaks this process into two stages, producing a logical screen from the domain data and then rendering it in HTML. There’s one first-stage view for each screen but only one second-stage view for the whole application.</p>
        <p>The advantage of the <i>Two Step View</i> (365) is that it puts the decision of what HTML to use in a single place. This makes global changes to the HTML easy since there’s only one object to alter in order to alter every screen on the site. Of course, you only get that advantage if your logical presentation stays the same, so it works best with sites where different screens use the same basic layout. Highly design intensive sites won’t be able to come up with a good logical screen structure. <i>Two Step View</i> (365) works even better if you have a Web application where its services are being used by multiple front-end customers, such as multiple airlines fronting the same basic reservation system. Within the limits of the logical screen, each front end can have a different appearance by using a different second stage. In a similar way you can use a Two Step View (365) to handle different output devices, with separate second stages for a regular Web browser and for a palmtop. Again, the limitation is that you can have the two share a common logical screen, which may not be possible if the UIs are very different, such as in a browser and a cell phone.</p>
        <h3 id="Input_Controller_Patterns">Input Controller Patterns</h3>
        <p>There are two patterns for the input controller. The most common is an input controller object for every page on your Web site. In the simplest case this <i>Page Controller</i> (333) can be a server page itself, combining the roles of view and input controller. In many implementations it makes things easier to split the input controller into a separate object. The input controller can then create appropriate models to do the processing and instantiate a view to return the result. Often you’ll find that there isn’t quite a one-to-one relationship between <i>Page Controllers</i> (333) and views. A more precise thought is that you have a <i>Page Controller</i> (333) for each action, where an action is a button or link. Most of the time the actions correspond to pages, but occasionally they don’t—such as a link that may go to a couple of different pages depending some condition. With any input controller there are two responsibilities—handling the HTTP request and deciding what to do with it—and it often makes sense to separate them. A server page can handle the request, delegating a separate helper object to decide what to do with it. <i>Front Controller</i> (344) goes further in this separation by having only one object handling all requests. This single handler interprets the URL to figure out what kind of request it’s dealing with and creates a separate object to process it. In this way you can centralize all HTTP handling within a single object, avoiding the need to reconfigure the Web server when ever you change the action structure of the site.</p>
        <h3 id="Further_Reading1">Further Reading</h3>
        <p>Most books on Web server technologies provide a chapter or two on good server designs, although these are often buried in the technological descriptions. An excellent discussion of Java Web design is Chapter 9 of [Brown et al.]. The best source for further patterns is [Alur et al.]; most of these patterns can be used in non-Java situations. I stole the terminology on separating input and application controllers from [Knight and Dai]. <i>This page intentionally left blank</i></p>
        <h3 id="Concurrency">Chapter 5</h3>
        <h2>Concurrency</h2>
        <p>Concurrency is one of the most tricky aspects of software development. Whenever you have multiple processes or threads manipulating the same data, you run into concurrency problems. Just thinking about concurrency is hard since it’s difficult to enumerate the possible scenarios that can get you into trouble. Whatever you do, there always seems to be something you miss. Furthermore, concurrency is hard to test for. We’re great fans of a large body of automated tests acting as a foundation for software development, but it’s hard to get tests to give us the security we need for concurrency problems. One of the great ironies of enterprise application development is that few branches of software development use concurrency more yet worry about it less. The reason enterprise developers can get away with a naive view of concurrency is transaction managers. Transactions provide a framework that helps avoid many of the most tricky aspects of concurrency in an enterprise application. As long as you do all your data manipulation within a transaction, nothing really bad will happen to you.</p>
        <p>Sadly, this doesn’t mean we can ignore concurrency problems completely, for the primary reason that many interactions with a system can’t be placed within a single database transaction. This forces us to manage concurrency in situations where data spans transactions. The term we use is <b>offline concurrency</b>, that is, concurrency control for data that’s manipulated during multiple database transactions. The second area where concurrency rears its ugly head for enterprise developers is application servers—supporting multiple threads in an application server system. We’ve spent much less time on this because dealing with it is much simpler. Indeed, you can use server platforms that take care of much of it for you.</p>
        <p>Sadly, to understand these issues, you need to understand at least some of the general concurrency concepts. So we begin this chapter by going over these issues. We don’t pretend that this chapter is a general treatment of concurrency in software development—for that we’d need at least a complete book. What this chapter does is introduce concurrency issues for enterprise application development. Once we’ve done that we’ll introduce the patterns for handling offline concurrency and say our brief words on application server concurrency. In much of this chapter we’ll illustrate the ideas with examples from an area that we hope you are very familiar with—the source code control systems used by teams to coordinate changes to a code base. We do this because it’s relatively easy to understand as well as well as familiar. After all, if you aren’t familiar with source code control systems, you really shouldn’t be developing enterprise applications.</p>
        <h3 id="Concurrency_Problems">Concurrency Problems</h3>
        <p>We’ll start by going through the essential problems of concurrency. We call them essential because they’re the fundamental problems that concurrency control systems try to prevent. They aren’t the only problems of concurrency, because the control mechanisms often create a new set of problems in theirsolutions! However, they do focus on the essential point of concurrency control.</p>
        <p><b>Lost updates</b> are the simplest idea to understand. Say Martin edits a file to make some changes to the checkConcurrency method—a task that takes a few minutes. While he’s doing this David alters the updateImportantParameter method in the same file. David starts and finishes his alteration very quickly, so quickly that, even though he starts after Martin, he finishes before him. This is unfortunate. When Martin read the file it didn’t include David’s update, so when Martin writes the file it writes over the version that David updated and David’s update is lost forever.</p>
        <p>An <b>inconsistent read</b> occurs when you read two things that are correct pieces of information but not correct at the same time. Say Martin wishes to know how many classes are in the concurrency package, which contains two subpackages for locking and multiphase. Martin looks in the locking package and sees seven classes. At this point he gets a phone call from Roy on some abstruse question. While Martin’s answering the phone, David finishes dealing with that pesky bug in the four-phase lock code and adds two classes to the locking package and three classes to the five that were in the multiphase package. His phone call over, Martin looks in the multiphase package to see how many classes there are and sees eight, producing a grand total of fifteen. Sadly, fifteen classes was never the right answer. The correct answer was twelve before David’s update and seventeen afterward. Either answer would have been correct, even if not current, but fifteen was never correct. This problem is called an inconsistent read because the data that Martin read was inconsistent.</p>
        <p>Both of these problems cause a failure of <b>correctness</b> (or safety), and theyresult in incorrect behavior that would not have occurred without two people trying to work with the same data at the same time. However, if correctness were the only issue, these problems wouldn’t be that serious. After all, we can arrange things so that only one of us can work the data at one time. While this helps with correctness, it reduces the ability to do things concurrently. The essential problem of any concurrent programming is that it’s not enough to worry about correctness; you also have to worry about <b>liveness</b>: how much concurrent activity can go on. Often people need to sacrifice some correctness to gain more liveness, depending on the seriousness and likelihood of the failures and the need for people to work on their data concurrently. These aren’t all the problems you get with concurrency, but we think of these as the basic ones. To solve them we use various control mechanisms. Alas, there’s no free lunch. The solutions introduce problems of their own, although these problems are less serious than the basic ones. Still, this does bring up an important point: If you can tolerate the problems, you can avoid any form of concurrency control. This is rare, but occasionally you find circumstances that permit it.</p>
        <h3 id="Execution_Contexts">Execution Contexts</h3>
        <p>Whenever processing occurs in a system, it occurs in some context and usually in more than one. There’s no standard terminology for execution contexts, so here we’ll define the ones that we’re assuming in this book. From the perspective of interacting with the outside world, two important contexts are the request and the session. A request corresponds to a single call from the outside world which the software works on and for which it optionally sends back a response. During a request the processing is largely in theserver’s court and the client is assumed to wait for a response. Some protocols allow the client to interrupt a request before it gets a response, but this is fairly rare. More often a client may issue another request that may interfere with one it just sent. So a client may ask to place an order and then issue a separate request to cancel that order. From the client’s view the two requests may be obviously connected, but depending on your protocol that may not be so obvious to the server.</p>
        <p>A <b>session</b> is a long-running interaction between a client and a server. It may consist of a single request, but more commonly it consists of a series of requests that the user regards as a consistent logical sequence. Commonly a session will begin with a user logging in and doing various bits of work that may involve issuing queries and one or more business transactions (to be discussed shortly). At the end of the session the user logs out, or he may just go away and assume that the system interprets that as logging out. Server software in an enterprise application sees both requests and sessions from two angles, as the server from the client and as the client to other systems. Thus, you’ll often see multiple sessions: HTTP sessions from the client and atabase sessions with various databases.</p>
        <p>Two important terms from operating systems are processes and threads. A process is a, usually heavyweight, execution context that provides a lot of isolation for the internal data it works on. A thread is a lighter-weight active agent that’s set up so that multiple threads can operate in a single <b>process</b>. People like threads because they support multiple requests within a single process—which is good utilization of resources. However, threads usually share memory, and such sharing leads to concurrency problems. Some environments allow you to control what data a thread may access, allowing you to have isolated threads that don’t share memory.</p>
        <p>The difficulty with execution contexts comes when they don’t line up as well as we might like. In theory each session would have an exclusive relationship with a process for its whole lifetime. Since processes are properly isolated from each other, this would help reduce concurrency conflicts. Currently we don’t know of any server tools that allow you to work this way. A close alternative is to start a new process for each request, which was the common mode for early Perl Web systems. People tend to avoid that now because starting processes tie up a lot of resources, but it’s quite common for systems to have a process handle only one request at a time—and that can save many concurrency headaches. When you’re dealing with databases there’s another important context—a transaction. Transactions pull together several requests that the client wants treated as if they were a single request. They can occur from the application to the database (a system transaction) or from the user to an application (a business transaction). We’ll dig into these terms more later on.</p>
        <h3 id="Isolation_and_Immutability">Isolation and Immutability</h3>
        <p>The problems of concurrency have been around for a while, and software people have come up with various solutions. For enterprise applications two solutions are particularly important: isolation and immutability. Concurrency problems occur when more than one active agent, such as a process or thread, has access to the same piece of data. One way to deal with this is isolation: Partition the data so that any piece of it can only be accessed by one active agent. Processes work like this in operating system memory: The operating system allocates memory exclusively to a single process, and only that process can read or write the data linked to it. Similarly you find file locks in many popular productivity applications. If Martin opens a file, nobody else can open it. They may be allowed to open a read-only copy of the file as it was when Martin started, but they can’t change it and they don’t get to see the file between his changes. Isolation is a vital technique because it reduces the chance of errors. Too often we’ve seen people get themselves into trouble because they use a technique that forces everyone to worry about concurrency all the time. With isolation you arrange things so that the program enters an isolated zone, within which you don’t have to worry about concurrency. Good concurrency design is thus to find ways of creating such zones and to ensure that as much programming as possible is done in one of them. You only get concurrency problems if the data you’re sharing can be modified. So one way to avoid concurrency conflicts is to recognize immutable data. Obviously we can’t make all data immutable, as the whole point of many systems is data modification. But by identifying some data as immutable, or at least immutable almost all the time, we can relax our concurrency concerns for it and share it widely. Another option is to separate applications that are only reading data, and have them use copied data sources, from which we can then relax all concurrency controls.</p>
        <h3 id="Optimistic_and_Pessimistic_Concurrency_Control">Optimistic and Pessimistic Concurrency Control</h3>
        <p>What happens when we have mutable data that we can’t isolate? In broad terms there are two forms of concurrency control that we can use: optimistic and pessimistic. Let’s suppose that Martin and David both want to edit the Customer file at the same time. With optimistic locking both of them can make a copy of the file and edit it freely. If David is the first to finish, he can check in his work without trouble. The concurrency control kicks in when Martin tries to commit his changes. At this point the source code control system detects a conflict between Martin’s changes and David’s changes. Martin’s commit is rejected and it’s up to him to figure out how to deal with the situation. With pessimistic locking whoever checks out the file first prevents anyone else from editing it. So if Martin is first to check out, David can’t work with the file until Martin is finished with it and commits his changes.</p>
        <p>A good way of thinking about this is that an optimistic lock is about conflict detection while a pessimistic lock is about conflict prevention. As it turns out\ real source code control systems can use either type, although these days most source code developers prefer to work with optimistic locks. (There is a reasonable argument that says that optimistic locking isn’t really locking, but we find the terminology too convenient, and widespread, to ignore.) Both approaches have their pros and cons. The problem with the pessimistic lock is that it reduces concurrency. While Martin is working on a file he locks it, so everybody else has to wait. If you’ve worked with pessimistic source code control mechanisms, you know how frustrating this can be. With enterprise data it’s often worse because, if someone is editing data, nobody else is allowed to read it, let alone edit it.</p>
        <p>Optimistic locks allow people to make much better progress, because the lock is only held during the commit. The problem with them is what happens when you get a conflict. Essentially everybody after David’s commit has to check out the version of the file that David checked in, figure out how to merge their changes with David’s changes, and then check in a newer version. With source code this happens not to be too difficult. Indeed, in many cases the source code control system can automatically do the merge for you, and even when it can’t automerge, tools can make it much easier to see the differences. But business data is usually too difficult to automerge, so often all you can do is throw away everything and start again. The essence of the choice between optimistic and pessimistic locks is the frequency and severity of conflicts. If conflicts are sufficiently rare, or if the consequences are no big deal, you should usually pick optimistic locks because they give you better concurrency and are usually easier to implement. However, if the results of a conflict are painful for users, you’ll need to use a pessimistic technique instead. Neither of these approaches is exactly free of problems. Indeed, by using them you can easily introduce problems that cause almost as much trouble as the basic concurrency problems that you’re trying to solve in the first place. We’ll leave a detailed discussion of these ramifications to a proper book on concurrency, but here are a few highlights to bear in mind</p>
        <h4 id="Preventing_Inconsistent_Reads">Preventing Inconsistent Reads</h4>
        <p>Consider this situation. Martin edits the Customer class, which makes calls on the Order class. Meanwhile David edits the Order class and changes the interface. David compiles and checks in; Martin then compiles and checks in. Now the shared code is broken because Martin didn’t realize that the Order class was altered underneath him. Some source code control systems will spot this inconsistent read, but others require some kind of manual discipline to enforce consistency, such as updating your files from the trunk before you check in. In essence this is the inconsistent read problem, and it’s often easy to miss because most people tend to focus on lost updates as the essential problem in concurrency. Pessimistic locks have a well-worn way of dealing with this problem through read and write locks. To read data you need a read (or shared) lock; to write data you need a write (or exclusive) lock. Many people can have read locks on the data at one time, but if anyone has a read lock nobody can get a write lock. Conversely, once somebody has a write lock, then nobody else can have any lock. With this system you can avoid inconsistent reads with pessimistic locks. Optimistic locks usually base their conflict detection on some kind of version marker on the data. This can be a timestamp or a sequential counter. To detect lost updates the system checks the version marker of your update with the version marker of the shared data. If they’re the same, the system allows the update and updates the version marker.</p>
        <p>Detecting an inconsistent read is essentially similar: In this case every bit of data that was read also needs to have its version marker compared with the shared data. Any differences indicate a conflict. Controlling access to every bit of data that’s read often causes unnecessary problems due to conflicts or waits on data that doesn’t actually matter that much. You can reduce this burden by separating out data you’ve used from data you’ve merely read. With a pick list of products it doesn’t matter if a new product appears in it after you start your changes. But a list of charges that you’re summarizing for a bill may be more important. The difficulty is that this requires some careful analysis of what it’s used for. A zip code in a customer’s address may seem innocuous, but, if a tax calculation is based on where somebody lives, that address has to be controlled for concurrency. As you can see, figuring out what you need to control and what you don’t is an involved exercise whichever form of concurrency control you use. Another way to deal with inconsistent read problems is to use Temporal Reads. These prefix each read of data with some kind of timestamp or immutable label, and the database returns the data as it was according to that time or label. Very few databases have anything like this, but developers often come across this in source code control systems. The problem is that the data source needs to provide a full temporal history of changes, which takes time and space to process. This is reasonable for source code but both more difficult and more expensive for databases. You may need to provide this capability for specific areas of your domain logic: see [Snodgrass] and [Fowler TP] for ideas on how to do that.</p>
        <h4 id="Deadlocks">Deadlocks</h4>
        <p>A particular problem with pessimistic techniques is deadlock. Say Martin starts editing the Customer file and David starts editing the Order file. David realizes that to complete his task he needs to edit the Customer file too, but Martin has a lock on it so he has to wait. Then Martin realizes he has to edit the Order file, which David has locked. They are now deadlocked—neither can make progress until the other completes. Described like this, deadlocks sound easy to prevent, but they can occur with many people involved in a complex chain, and that makes them more tricky. There are various techniques you can use to deal with deadlocks. One is to have software that can detect a deadlock when it occurs. In this case you pick a victim, who has to throw away his work and his locks so the others can make progress. Deadlock detection is very difficult and causes pain for victims. A similar approach is to give every lock a time limit. Once you hit that limit you lose your locks and your work—essentially becoming a victim. Timeouts are easier to implement than a deadlock detection mechanism, but if anyone holds locks for a while some people will be victimized when there actually is no deadlock present.</p>
        <p>Timeouts and detection deal with a deadlock when it occurs, other approaches try to stop deadlocks occurring at all. Deadlocks essentially occur when people who already have locks try to get more (or to upgrade from read o write locks.) Thus, one way of preventing them is to force people to acquire all their locks at once at the beginning of their work and then prevent them aining more. You can force an order on how everybody gets locks. An example might be to always get locks on files in alphabetical order. This way, once David had a lock on the Order file, he can’t try to get a lock on the Customer file because it’s earlier in the sequence. At that point he essentially becomes a victim. You can also make it so that, if Martin tries to acquire a lock and David already has one, Martin automatically becomes a victim. It’s a drastic technique, but it’s simple to implement. And in many cases such a scheme works just fine.</p>
        <p>If you’re very conservative, you can use multiple schemes. For example, you force everyone to get all their locks at the beginning, but add a timeout in case something goes wrong. That may seem like using a belt and braces, but such conservatism is often wise with deadlocks because they are pesky things that are easy to get wrong. It’s very easy to think you have a deadlock-proof scheme and then find some chain of events you didn’t consider. As a result we prefer very simple and con- servative schemes for enterprise application development. They may cause unnecessary victims, but that’s usually much better than the consequences of missing a deadlock scenario.</p>
        <h3 id="Transactions">Transactions</h3>
        <p>The primary tool for handling concurrency in enterprise applications is the transaction. The word “transaction” often brings to mind an exchange of money or goods. Walking up to an ATM machine, entering your PIN, and withdrawing cash is a transaction. Paying the $3 toll at the Golden Gate Bridge is a transaction. Buying a beer at the local pub is a transaction. Looking at typical financial dealings such as these provides a good definition for the term. First, a transaction is a bounded sequence of work, with both startand endpoints well defined. An ATM transaction begins when the card is inserted and ends when cash is delivered or an inadequate balance is discovered. Second, all participating resources are in a consistent state both when the transaction begins and when the transaction ends. A man purchasing a beer has a few bucks less in his wallet but has a nice pale ale in front of him. The sum value of his assets hasn’t changed. It’s the same for the pub—pouring free beer would be no way to run a business. In addition, each transaction must complete on an all-or-nothing basis. The bank can’t subtract from an account holder’s balance unless the ATM machine actually delivers the cash. While the human element might make this last property optional during the above transactions, there is no reason software can’t make a guarantee on this front.</p>
        <h4 id="ACID">ACID</h4>
        <p>Software transactions are often described in terms of the ACID properties:
                <ul>
                        <li><b>Atomicity</b>: Each step in the sequence of actions performed within the boundaries of a transaction must complete successfully or all work must roll back. Partial completion is not a transactional concept. Thus, if Martin is transferring some money from his savings to his checking account and the server crashes after he’s withdrawn the money from his savings, the system behaves as if he never did the withdrawal. Committing says both things occurred; a roll back says neither occurred. It has to be both or neither.</li>
                        <li><b>Consistency</b>: A system’s resources must be in a consistent, noncorrupt state at both the start and the completion of a transaction.</li>
                        <li><b>Isolation</b>: The result of an individual transaction must not be visible to any other open transactions until that transaction commits successfully.</li>
                        <li><b>Durability</b>: Any result of a committed transaction must be made permanent. This translates to “Must survive a crash of any sort.”</li>
                </ul>
        </p>
        <h3 id="Transactional_Resources">Transactional Resources</h3>
        <p>Most enterprise applications run into transactions in terms of databases. But there are plenty of other things that can be controlled using transactions, such as message queues, printers, and ATMs. As a result, technical discussions of transactions use the term “transactional resource” to mean anything that’s transactional—that is, that uses transactions to control concurrency. “Transactional resource” is a bit of a mouthful, so we just use “database,” since that’s the most common case. But when we say “database,” the same applies for any other transactional resource. To handle the greatest throughput, modern transaction systems are designed to keep transactions as short as possible. As a result the general advice is to never make a transaction span multiple requests. A transaction that spans multiple requests is generally known as a long transaction. For this reason a common approach is to start a transaction at the beginning of a request and complete it at the end. This request transaction is a nice simple model, and a number of environments make it easy to do declaratively, by just tagging methods as transactional.</p>
        <p>A variation on this is to open a transaction as late as possible. With a late transaction you may do all the reads outside it and only open it up when you do updates. This has the advantage of minimizing the time spent in a transaction. If there’s a lengthy time lag between the opening of the transaction and the first write, this may improve liveness. However, this means that you don’t have any concurrency control until you begin the transaction, which leaves you open to inconsistent reads. As a result it’s usually not worth doing this unless you have very heavy contention or you’re doing it anyway because of business transactions that span multiple requests (which is the next topic). When you use transactions, you need be somewhat aware of what exactly is being locked. For many database actions the transaction system locks the rows involved, which allows multiple transactions to access the same table. However, if a transaction locks a lot of rows in a table, then the database has more locks than it can handle and escalates the locking to the entire table—locking out other transactions. This lock escalation can have a serious effect on concurrency, and it’s particularly why you shouldn’t have some “object” table for data at the domain’s Layer Supertype (475) level. Such a table is a prime candidate for lock escalation, and locking that table shuts everybody else out of the database.</p>
        <h4 id="Reducing_Transaction_Isolation_for_Liveness">Reducing Transaction Isolation for Liveness</h4>
        <p>It’s common to restrict the full protection of transactions so that you can get better liveness. This is particularly the case when it comes to handling isolation. If you have full isolation, you get serializable transactions. Transactions are serializable if they can be executed concurrently and you get a result that’s the same as you get from some sequence of executing the transactions serially. Thus, if we take our earlier example of Martin counting his files, serializability guarantees that he gets a result that corresponds to completing his transaction either entirely before David’s transaction starts (twelve) or entirely after David’s finishes (seventeen). Serializability can’t guarantee which result, as in this case, but at least it guarantees a correct one. Most transactional systems use the SQL standard which defines four levels of isolation. Serializable is the strongest level, and each level below allows a particular kind of inconsistent read to enter the picture. We’ll explore these with he example of Martin counting files while David modifies them. There are two packages: locking and multiphase. Before David’s update there are seven files in the locking package and five in the multiphase package; after his update there are nine in the locking package and eight in the multiphase package. Martin looks at the locking package and David then updates both; then Martin looks at the multiphase package.</p>
        <p>If the isolation level is serializable, the system guarantees that Martin’s answer is either twelve or seventeen, both of which are correct. Serializability can’t guarantee that every run through this scenario will give the same result, but it always gets either the number before David’s update or the number afterwards. The first isolation level below serializable is repeatable read, which allows phantoms. Phantoms occur when you add some elements to a collection and the reader sees only some of them. The case here is that Martin looks at the files in the locking package and sees seven. David then commits his transaction, after which Martin looks at the multiphase package and sees eight. Hence, Martin gets an incorrect result. Phantoms occur because they are valid for some of Martin’s transaction but not all of it, and they’re always things that are inserted.</p>
        <p>Next down the list is the isolation level of read committed, which allows unrepeatable reads. Imagine that Martin looks at a total rather than the actual files. An unrepeatable read allows him to read a total of seven for locking. Next David commits; then he reads a total of eight for multiphase. It’s called an unrepeatable read because, if Martin were to reread the total for the locking package after David committed, he would get the new number of nine. His original read of seven can’t be repeated after David’s update. It’s easier for databases to spot unrepeatable reads than phantoms, so the repeatable read gives you more correctness than read committed but less liveness. The lowest level of isolation is read uncommitted, which allows dirty reads. At read uncommitted you can read data that another transaction hasn’t actually committed yet. This causes two kinds of errors. Martin might look at the locking package when David adds the first of his files but before he adds the second. As a result he sees eight files in the locking package. The second kind of error comes if David adds his files but then rolls back his transaction—in which case Martin sees files that were never really there. Table 5.1 lists the read errors caused by each isolation level. To be sure of correctness you should always use the serializable isolation level. The problem is that choosing serializable really messes up the liveness of a system, so much so that you often have to reduce serializability in order to increase throughput. You have to decide what risks you want to take and make your own trade-off of errors versus performance. You don’t have to use the same isolation level for all transactions, so you should look at each transaction and decide how to balance liveness versus correctness for it. </p>
        <h4 id="Business_and_System_Transactions">Business and System Transactions</h4>
        <p>What we’ve talked about so far, and most of what most people talk about, is what we call system transactions, or transactions supported by RDBMS systems and transaction monitors. A database transaction is a group of SQL commands delimited by instructions to begin and end it. If the fourth statement in the transaction results in an integrity constraint violation, the database must roll back the effects of the first three statements and notify the caller that the transaction has failed. If all four statements had completed successfully all would have been made visible to other users at the same time rather than one at a time. RDBMS systems and application server transaction managers are so commonplace that they can pretty much be taken for granted. They work well and are well understood by application developers. However, a system transaction has no meaning to the user of a business system. To an online banking system user a transaction consists of logging in, selecting an account, setting up some bill payments, and finally clicking the OK button to pay the bills. This is what we call a business transaction, and that it displays the same ACID properties as a system transaction seems a reasonable expectation. If the user cancels before paying the bills, any changes made on previous screens should be canceled. Setting up payments shouldn’t result in a                system-visible balance change until the OK button is pressed.</p>
        <p>The obvious answer to supporting the ACID properties of a business transaction is to execute the entire business transaction within a single system transaction. Unfortunately business transactions often take multiple requests to complete, so using a single system transaction to implement one results in a long system transaction. Most transaction systems don’t work very efficiently with long transactions. This doesn’t mean that you should never use long transactions. If your database has only modest concurrency needs, you may well be able to get away with it. And if you can get away with it, we suggest you do it. Using a long transaction means you avoid a lot of awkward problems. However, the application won’t be scalable because long transactions will turn the database into a major bottleneck. In addition, the refactoring from long to short transactions is both complex and not well understood. For this reason many enterprise applications can’t risk long transactions. In this case you have to break the business transaction down into a series of short transactions. This means that you are left to your own devices to support the ACID properties of business transactions between system transactions—a problem we call offline concurrency. System transactions are still very much part of the picture. Whenever the business transaction interacts with a transactional resource, such as a database, that interaction will execute within a system transaction in order to maintain the integrity of that resource. However, as you’ll read below it’s not enough to string together a series of system transactions to properly support a business transaction. The business application must provide a bit of glue between them.</p>
        <p>Atomicity and durability are the ACID properties most easily supported for business transactions. Both are supported by running the commit phase of the business transaction, when the user hits Save, within a system transaction. Before the session attempts to commit all its changes to the record set, it first opens a system transaction. The system transaction guarantees that the changes will commit as a unit and will be made permanent. The only potentially tricky part here is maintaining an accurate change set during the life of the business transaction. If the application uses a Domain Model (116), a Unit of Work (184) can track changes accurately. Placing business logic in a Transaction Script (110) requires a manual tracking of changes, but that’s probably not much of a problem as the use of transaction scripts implies rather simple business transactions. The tricky ACID property to enforce with business transactions is isolation. Failures of isolation lead to failures of consistency. Consistency dictates that a business transaction not leave the record set in an invalid state. Within a single transaction the application’s responsibility in supporting consistency is to enforce all available business rules. Across multiple transactions the application’s responsibility is to ensure that one session doesn’t step all over another session’s changes, leaving the record set in the invalid state of having lost a user’s work.</p>
        <p>As well as the obvious problems of clashing updates, there are the more subtle problems of inconsistent reads. When data is read over several system transactions, there’s no guarantee that it will be consistent. The different reads can even introduce data in memory that’s sufficiently inconsistent to cause application failures. Business transactions are closely tied to sessions. In the user’s view each session is a sequence of business transactions (unless they’re only reading data), so we usually make the assumption that all business transactions execute in a single client session. While it’s certainly possible to design a system that has multiple sessions for one business transaction, that’s a very good way of getting yourself badly confused—so we’ll assume that you won’t do that.</p>
        <h3 id="Patterns_for_Offline_Concurrency_Control">Patterns for Offline Concurrency Control</h3>
        <p>As much as possible, you should let your transaction system deal with concurrency problems. Handling concurrency control that spans system transactions plonks you firmly in the murky waters of dealing with concurrency yourself. This water is full of virtual sharks, jellyfish, piranhas, and other, less friendly creatures. Unfortunately, the mismatch between business and system transactions means you sometimes just have to wade in. The patterns that we’ve provided here are some techniques that we’ve found helpful in dealing with concurrency control that spans system transactions. Remember that these are techniques you should only use if you have to. If you can make all your business transactions fit into a system transaction by ensuring that they fit within a single request, then do that. If you can get away ith long transactions by forsaking scalability, then do that. By leaving concurrency control in the hands of your transaction software you’ll avoid a great deal of trouble. These techniques are what you have to use when you can’t do that. Because of the tricky nature of concurrency, we have to stress again that the patterns are a starting point, not a destination. We’ve found them useful, but we don’t claim to have found a cure for all concurrency ills.</p>
        <p>Our first choice for handling offline concurrency problems is Optimistic <i>Offline Lock</i> (416), which essentially uses optimistic concurrency control across the business transactions. We like this as a first choice because it’s an easier approach to program and yields the best liveness. The limitation of Optimistic Offline Lock (416) is that you only find out that a business transaction is going to fail when you try to commit it, and in some circumstances the pain of that late discovery is too much. Users may have put an hour’s work into entering details about a lease, and if you get lots of failures users lose faith in the system. Your alternative is Pessimistic Offline Lock (426), with which you find out early if you’re in trouble but lose out because it’s harder to program and it reduces your liveness. With either of these approaches you can save considerable complexity by not trying to manage locks on every object. A <i>Coarse-Grained Lock</i> (438) allows you to manage the concurrency of a group of objects together. Another way you can make life easier for application developers is to use <i>Implicit Lock</i> (449), which saves them from having to manage locks directly. Not only does this save work, it also avoids bugs when people forget—and these bugs are hard to find. A common statement about concurrency is that it’s a purely technical decision that can be decided on after requirements are complete. We disagree. The choice of optimistic or pessimistic controls affects the whole user experience of the system. An intelligent design of <i>Pessimistic Offline Lock</i> (426) needs a lot of input about the domain from the users of the system. Similarly domain knowledge is needed to choose good Coarse-Grained Locks (438). Futzing with concurrency is one of the most difficult programming tasks. It’s ery difficult to test concurrent code with confidence. Concurrency bugs are hard to reproduce and very difficult to track down. The patterns we’ve described have worked for us so far, but this is particularly difficult territory. If you need to go down this path, it’s worth getting some experienced help. At the very least consult the books we mention at the end of this chapter.</p>
        <h3 id="Application_Server_Concurrency">Application Server Concurrency</h3>
        <p>So far we’ve talked about concurrency mainly in terms of multiple sessions running against a shared data source. Another form of concurrency is the process concurrency of the application server itself: How does that server handle multiple requests concurrently and how does this affect the design of the application on the server? The big difference from the other concurrency issues we’ve talked about so far is that application server concurrency doesn’t involve transactions, so working with them means stepping away from the relatively controlled transactional world. Explicit multithreaded programming, with locks and synchronization blocks, is complicated to do well. It’s easy to introduce defects that are very hard to find—concurrency bugs are almost impossible to reproduce—resulting in a system that works correctly 99 percent of the time but throws random fits. Such software is incredibly frustrating to use and debug, so our policy is to avoid the need for explicit handling of synchronization and locks as much as possible. Application developers should almost never have to deal with these explicit concurrency mechanisms.</p>
        <p>The simplest way to handle this is to use process-per-session, where each session runs in its own process. Its great advantage is that the state of each process is completely isolated from the other processes so application programmers don’t have to worry at all about multithreading. As far as memory isolation goes, it’s almost equally effective to have each request start a new process or to have one process tied to the session that’s idle between requests. Many early Web systems would start a new Perl process for each request. The problem with process-per-session is that it uses up a lot resources, since processes are expensive beasties. To be more efficient you can pool the processes, such that each one only handles a single request at one time but can handle multiple requests from different sessions in a sequence. This approach of pooled process-per-request will use many fewer processes to support a given number of sessions. Your isolation is almost as good: You don’t have many of the nasty multithreading issues. The main problem of process for request over process-persession is that you have to ensure that any resources used to handle a request are released at the end of the request. The current Apache mod-perl uses this scheme, as do a lot of serious large-scale transaction processing systems. Even process-per-request will need many processes running to handle a reasonable load. You can further improve throughput by having a single process run multiple threads. With this thread-per-request approach, each request is handled by a single thread within a process. Since threads use much fewer server resources than a process, you can handle more requests with less hardware this way, so your server is more efficient. The problem with using threadper-request is that there’s no isolation between the threads and any thread can touch any piece of data that it can get access to.</p>
        <p>In our view there’s a lot to be said for using process-per-request. Although it’s less efficient than thread-per-request, using process-per-request is equally scalable. You also get better robustness—if one thread goes haywire it can bring down an entire process, so using process-per-request limits the damage. Particularly with a less experienced team, the reduction of threading headaches (and the time and cost of fixing bugs) is worth the extra hardware costs. We find that few people actually do any performance testing to assess the relative costs of thread-per-request and process-per-request for their application. Some environments provide a middle ground of allowing isolated areas of data to be assigned to a single thread. COM does this with the single-threaded apartment, and J2EE does it with Enterprise Java Beans (and will in the future with isolates). If your platform has something like this available, it can allow you to have your cake and eat it—whatever that means.</p>
        <p>If you use thread-per-request, the most important thing is to create and enter an isolated zone where application developers can mostly ignore multithreaded issues. The usual way to do this is to have the thread create new objects as it starts handling the request and to ensure that these objects aren’t put anywhere (such as in a static variable) where other threads can see them. That way the objects are isolated because other threads have no way of referencing them. Many developers are concerned about creating new objects because they’ve been told that object creation is an expensive process. As a result they often pool objects. The problem with pooling is that you have to synchronize access to the pooled objects in some way. But the cost of object creation is very dependent on the virtual machine and memory management strategies. In modern environments object creation is actually pretty fast [Peckish]. (Off the top of your head: how many Java date objects do you think we can create in one second on Martin’s 600Mhz P3 with Java 1.3? We’ll tell you later.) Creating fresh objects for each session avoids a lot of concurrency bugs and can actually improve scalability.</p>
        <p>While this tactic works in many cases, there are still some areas that developers need to avoid. One is static, class-based variables or global variables because any use of these has to be synchronized. This is also true of singletons. If you need some kind of global memory, use a Registry (480), which you can implement in such a way that it looks like a static variable but actually uses hread-specific storage. Even if you’re able to create objects for the session, and thus make a comparatively safe zone, some objects are expensive to create and thus need to be handled differently—the most common example of this is a database connection. To deal with this you can place these objects in an explicit pool where you acquire a connection while you need it and return it when done. These operations will need to be synchronized.</p>
        <h3 id="Further_Reading80">Further Reading</h3>
        <p>In many ways, this chapter only skims the surface of a much more complex topic. To investigate further we suggest starting with [Bernstein and Newcomer], [Lea], and [Schmidt et al.].</p>
        <h3 id="Session_State">Chapter 6</h3>
        <h2>Session State</h2>
        <p>When we talked about concurrency, we raised the issue of the difference between business and system transactions (Chapter 5, page 74). As well as affecting concurrency, this difference affects how to store the data that’s usedtechnical questions of stateless and stateful server systems. I think the fundamental issue is realizing that some sessions are inherently stateful and then deciding what to do about the state.</p>
        <h3 id="The_Value_of_Statelessness">The Value of Statelessness</h3>
        <p>What do people mean by a stateless server? The whole point of objects, of course, is that they combine state (data) with behavior. A true stateless object is one with no fields. Such animals do show up from time to time, but frankly, they’re pretty rare. Indeed, you can make a strong case that a stateless object is a bad design.</p>
        <p>As it turns out, however, this isn’t what most people mean when they talk about statelessness in a distributed enterprise application. When people refer to a stateless server they mean an object that doesn’t retain state between requests. Such an object may well have fields, but when you invoke a method on a stateless server the values of the fields are undefined. An example of a stateless server object might be one that returns a Web page telling you all about a book. You invoke a call on it by accessing a URL—the object might be an ASP document or a servlet. In the URL you supply an ISBN number that the server uses to generate the HTTP reply. During the interaction the server object might stash the book’s ISBN, title, and price in fields when it gets them back from the database, before it generates the HTML; maybe it does some business logic to determine which complimentary reviews to show the user. Once it’s done its job, however, these values become useless. The next ISBN is a whole new story, and the server object will probably reinitialize to clear out any old values to avoid mistakes.</p>         <p>Now imagine that you want to keep track of all the ISBNs visited by a particular client IP address. You can keep this in a list maintained by the server object. However, this list must persist between requests and thus you have a stateful server object. The shift from stateless to stateful is much more than three or four letters at the end of the word. For many people stateful servers are nothing short of disastrous. Why is this? The primary issue is one of server resources. Any stateful server object needs to keep all its state while waiting for a user to ponder a Web page. A stateless server object, however, can process other requests from other sessions. Here’s a completely unrealistic yet helpful thought experiment. We have a hundred people who want to know about books, and processing a request about a book takesone second. Each person makes one request every ten seconds, and all requests are perfectly balanced. If we want to track a user’s requests with a stateful server object, we must have one server object per user: one hundred objects. But 90 percent of the time these objects are sitting around doing nothing. If we forgo the ISBN tracking and just use stateless server objects to respond to requests, we can get away with only ten server objects fully employed all the time.</p>
        <p>The point is that, if we have no state between method calls, it doesn’t matter which object services the request, but if we do store state we need to always get the same object. Statelessness allows us to pool our objects so that we need fewer objects to handle more users. The more idle users we have, the more valuable stateless servers are. As you can imagine, stateless servers are very useful on high-traffic Web sites. Statelessness also fits in well with the Web since HTTP is a stateless protocol. So everything should be stateless, right? Well, it would be if it could be. The problem is that many client interactions are inherently stateful. Consider the shopping cart metaphor that fuels a thousand e-commerce applications. Theuser’s interaction involves browsing several books and picking which ones to buy. The shopping cart needs to be remembered for the user’s entire session. Essentially we have a stateful business transaction, which implies that the session has to be stateful. If I only look at books and don’t buy anything, my session is stateless, but if I buy, it’s stateful. We can’t avoid the state unless we stay poor; instead, we have to decide what to do with it. The good news is that we can use a stateless server to implement a stateful session; the interesting news is that we may not want to.</p>
        <h3 id="Session_State83">Session State</h3>
        <p>The details of the shopping cart are session state, meaning that the data in thecart is relevant only to that particular session. This state is within a business transaction, which means that it’s separated from other sessions and their business transactions. (I’ll continue to assume for this discussion that each business transaction runs in one session only and that each session does only one business transaction at any one time). Session state is distinct from what I call record data, which is the long-term persistent data held in the database and visible to all sessions. Session state needs to be committed to become record data. Since session state is within a business transaction, it has many of the properties that people usually think of with transactions, such as ACID (atomicity, consistency, isolation, and durability). The consequences of this are not always understood.</p>
        <p>One interesting consequence is the effect on consistency. While the customer is editing an insurance policy, the current state of the policy may not be legal. The customer alters a value, uses a request to send this to the system, and the system replies indicating invalid values. Those values are part of the session state, but they aren’t valid. Session state is often like this—it isn’t going to match the validation rules while it’s being worked on; it will only when the business transaction commits.</p>
        <p>The biggest issue with session state is dealing with isolation. With many fingers in the pot, a number of things can happen while a customer is editing a policy. The most obvious is two people editing the policy at the same time. But it’s not just direct changes that are a problem. Consider that there are two records, the policy itself and the customer record. The policy has a risk value that depends partially on the zip code in the customer record. The customer begins by editing the policy and after ten minutes does something that opens the customer record so he can see the zip code. However, during that time someone else has changed the zip code and the risk value—leading to an inconsistent read. See page 76 for advice on how to deal with this. Not all data held by the session counts as session state. The session may cache some data that doesn’t really need to be stored between requests but is stored to improve performance. Since you can lose the cache without losing correct behavior, this is different from session state, which must be stored between requests for correct behavior.</p>
        <h4 id="Ways_to_Store_Session_State">Ways to Store Session State</h4>
        <p>So, how do you store session state once you know you have to have it? I divide the options into three blurred but basic choices.</p>
        <p><i>Client Session State</i> (456) stores the data on the client. There are several ways to do this: encoding data in a URL for a Web presentation, using cookies, serializing the data into some hidden field on a Web form, and holding the data in objects on a rich client.</p>
        <p><i>Server Session State</i> (458) may be as simple as holding the data in memory between requests. Usually, however, there’s a mechanism for storing the session state somewhere more durable as a serialized object. The object can be stored on the application server’s local file system, or it can be placed in a shared data source. This could be a simple database table with a session ID as a key and a serialized object as a value.</p>
        <p><i>Database Session State</i> (462) is also server-side storage, but it involves breaking up the data into tables and fields and storing it in the database much as you would store more lasting data. There are quite a few issues involved in the choice of option. First off, I’ll talk about bandwidth needs between the client and the server. Using Client Session State (456) means that session data needs to be transferred across the wire with every request. If we’re talking about only a few fields, this is no big deal, but larger amounts of data result in bigger transfers. In one application this data amounted to about a megabyte or, as one of our team put it, three Shakespeare plays worth. Admittedly, we were using XML between the two, which is not the most compact of data transmission forms, but even so there was a lot of data to work with.</p>
        <p>Of course, some data will need to be transferred because it has to be seen on the presentation. But using <i>Client Session State</i> (456) implies that with every request you have to transfer all the data the server uses for it, even if it isn’t needed by the client for display. All in all this means that you don’t want to use Client Session State (456) unless the amount of session state you need to store is pretty small. You also have to worry about security and integrity. Unless you encrypt the data, you have to assume that any malicious user could edit your session data, which might lead you to a whole new version of “name your own price.”</p>
        <p>Session data has to be isolated. In most cases what’s going on in one session shouldn’t affect what’s going on in another. If we book a flight itinerary there should be no effect on any other user until the flight is confirmed. Indeed, part of the meaning of session data is that it’s unseen to anything outside the session. This becomes a tricky issue if you use Database Session State (462), because you have to work hard to isolate the session data from the record data that sits in the database.</p>
        <p>If you have a lot of users, you’ll want to consider clustering to improve your throughput. In this case you’ll want to think about whether you need session migration. Session migration allows a session to move from server to server as one server handles one request and other servers take on the others. Its opposite is server affinity, which forces one server to handle all requests for a particular session. Server migration leads to a better balancing of your servers, particularly if your sessions are long. However, that can be awkward if you’re using Server Session State (458) because often only the machine that handles the session can easily find that state.There are ways around that— ways that blur the lines between Database Session State (462) and Server Session State (458). Server affinity can lead to bigger problems than you might initially think. In trying to guarantee server affinity, the clustering system can’t always inspect the calls to see which session they’re part of. As a result, it will increase the affinity so all calls from one client go to the same application server. Often this is done by the client’s IP address. If the client is behind a proxy, that could mean that many clients are all using the same IP address and are thus tied to a particular server. This can get pretty bad if you see most of your traffic handled by one server that bags the IP address for AOL!</p>
        <p>If the server is going to use the session state, it needs to get it into a form that can be used quickly. If you use Server Session State (458), the session state is pretty much there. If you use Client Session State (456), it’s there, but often needs to be put into the form you want. If you use Database Session State (462), you need to go to the database to get it (and maybe do some transforming as well). This implies that each approach can have different effects on thesystem’s responsiveness. The size and complexity of the data will have an effect on this time. If you have a public retail system, you probably don’t have that much data going into each session, but you do have a lot of mostly idle users. For that reason Database Session State (462) can work nicely in performance terms. For a leasing system you run the risk of schlepping masses of data in and out of the database with each request. That’s when Server Session State (458) can give youbetter performance.</p>
        <p>One of the big bugbears in many systems is when a user cancels a session and says forget it. This is particularly awkward with B2C applications because the user usually doesn’t actually say forget it, it just disappears and doesn’t comeback. Client Session State (456) certainly wins here because you can forget about that user easily. In the other approaches you need to be able to clean out session state when you realize it’s canceled, as well as set up a system that allows you to cancel after some timeout period. Good implementations of Server Session State (458) allow you to do this with an automatic timeout. As well as what happens when a user cancels, consider what happens when a system cancels: A client can crash, a server can go south, and a network connection can disappear into the ether. Database Session State (462) can usually cope with all three pretty well. Server Session State (458) may or may not survive, depending on whether the session object is backed up to a nonvolatile store and where that store is kept. Client Session State (456) won’t survive a client crash, but should survive the rest going down.</p>
        <p>Don’t forget the development effort involved in these patterns. Usually the Server Session State (458) is the easiest on development resources, particularly if you don’t have to persist the session state between requests. Database Session State (462) and Client Session State (456) will usually involve code to transform from a database or transport format to the form that the session objects will use. That extra time means that you aren’t able to build as many features as quickly with as you would with Server Session State (458), particularly if the data is complex. On first sight Database Session State (462) might not seem that complex if you’ve already got to map to database tables, but the extra development effort comes in keeping all the other uses of the database isolated from the session data.</p>
        <p>The three approaches aren’t mutually exclusive. You can use a mix of two or three of them to store different parts of the session state. This usually makes things more complicated, however, as you’re never sure which part of the state goes in what part of the system. Nevertheless, if you use something other than Client Session State (456), you’ll have to keep at least a session identifier in Client Session State (456) even if the rest of the state is held using the other patterns. My preference is for Server Session State (458), particularly if the memento is stored remotely so it can survive a server crash. I also like Client Session State (456) for session IDs and for session data that’s very small. I don’t like Database Session State (462) unless you need failover and clustering and if you can’t  store remote mementos or if isolation between sessions isn’t an issue for you.</p>
        <h3 id="Distribution_Strategies">Chapter 7</h3>
        <h2>Distribution Strategies</h2>
        <p>Objects have been around for a while, and sometimes it seems that, ever since they were created, folks have wanted to distribute them. However, distribution of objects, or indeed of anything else, has a lot more pitfalls than many people realize [Waldo et al.], especially when they’re under the influence of vendors’ cozy brochures. This chapter is about some of these hard lessons—lessons I’ve seen many of my clients learn the hard way.</p>
        <h3 id="The_Allure_of_Distributed_Objects">The Allure of Distributed Objects</h3>
        <p>There is a recurring presentation that I used to see two or three times a year during design reviews. Proudly the system architect of a new OO system lays out his plan for a new distributed object system—let’s pretend it’s a some kind of ordering system. He shows me a design that looks rather like Figure 7.1. With separate remote objects for customers, orders, products, and deliveries. Each one is a separate component that can be placed on a separate processing node. I ask, “Why do you do this?” “Performance, of course,” the architect replies, looking at me a little oddly. “We can run each component on a separate box. If one component gets too busy we add extra boxes for it so we can load-balance our application.” The look is now curious as if he wonders if I really know anything about real distributed object stuff at all. Meanwhile I’m faced with an interesting dilemma. Do I just say out and out that this design sucks like an inverted hurricane and get shown the door immediately? Or do I slowly try to show my client the light? The latter is more remunerative but much tougher since the client is usually very pleased with his architecture, and it takes a lot to give up on a fond dream.</p>
        <p>So assuming you haven’t shown this book the door I guess you’ll want to know why this distributed architecture sucks. After all, many tool vendors will tell you that the whole point of distributed objects is that you can take a bunch of objects and position them as you like on processing nodes. Also, their powerful middleware provides transparency. Transparency allows objects to call each other within a process or between a process without having to know if the callee is in the same process, in another process, or on another machine. Transparency is valuable, but while many things can be made transparent in distributed objects, performance isn’t usually one of them. Although our prototypical architect was distributing objects the way he was for performance reasons, in fact his design will usually cripple performance, make the system much harder to build and deploy, or, usually, do both. </p>
        <h3 id="Remote_and_Local_Interfaces">Remote and Local Interfaces</h3>
        <p>The primary reason that the distribution by class model doesn’t work has to do with a fundamental fact of computers. A procedure call within a process is very, very fast. A procedure call between two separate processes is orders of magnitude slower. Make that a process running on another machine and you can add another order of magnitude or two, depending on the network topography involved. As a result, the interface for an object to be used remotely must be different rom that for an object used locally within the same process. A local interface is best as a fine-grained interface. Thus, if I have an address  lass, a good interface will have separate methods for getting the city, getting the state, setting the city, setting the state, and so forth. A fine-grained interface is good because it follows the general OO principle of lots of little pieces that can be combined and overridden in various ways to extend the design into the future.</p>
        <p>A fine-grained interface doesn’t work well when it’s remote. When method calls are slow, you want to obtain or update the city, state, and zip in one call rather than three. The resulting interface is coarse-grained, designed not for flexibility and extendibility but for minimizing calls. Here you’ll see an interface\ along the lines of get-address details and update-address details. It’s much more awkward to program to, but for performance you need to have it. Of course, what vendors will tell you is that there’s no overhead to using their middleware for remote and local calls. If it’s a local call, it’s done with the speed of a local call. If it’s a remote call it’s done more slowly. Thus, you only pay the price of a remote call when you need one. This much is, to some extent, true, but it doesn’t avoid the essential point that any object that may be used remotely should have a coarse-grained interface while every object that isn’t used remotely should have a fine-grained interface. Whenever two objects communicate you have to choose which to use. If the object could ever be in separate processes you have to use the coarse-grained interface and pay the cost of the harder programming model. Obviously, it only makes sense to pay that cost when you need to, and so you need to minimize the amount of inter-process collaborations.</p>
        <p>For these reasons you can’t just take a group of classes that you design in the world of a single process, throw CORBA or some such at them, and come up with a distributed model. Distribution design is more than that. If you base your distribution strategy on a classes, you’ll end up with a system that does a lot of remote calls and thus needs awkward coarse-grained interfaces. In the end, even with coarse-grained interfaces on every remotable class, you’ll still end up with too many remote calls and a system that’s awkward to modify as a bonus.</p>
        <p>Hence, we get to my First Law of Distributed Object Design: Don’t distribute your objects! How, then, do you effectively use multiple processors? In most cases the way to go is clustering (see Figure 7.2). Put all the classes into a single process and then run multiple copies of that process on the various nodes. That way each process uses local calls to get the job done and thus does things faster. You can also use fine-grained interfaces for all the classes within the process and thus get better maintainability with a simpler programming model.</p>
        <h3 id="Where_You_Have_to_Distribute">Where You Have to Distribute</h3>
        <p>So you want to minimize distribution boundaries and utilize your nodes through clustering as much as possible. The rub is that there are limits to that approach—that is, places where you need to separate the processes. If you’re sensible, you’ll fight like a cornered rat to eliminate as many of them as you can, but you won’t eliminate them all.
                <ul>
                        <li>One obvious separation is between the traditional clients and servers of business software. PCs on users’ desktops are different nodes to shared repositories of data. Since they’re different machines you need separate processes that communicate. The client–server divide is a typical inter-process divide.</li>
                        <li>A second divide often occurs between server-based application software (the application server) and the database. Of course, you don’t have to do this. You can run all your application software in the database process it self using such things as stored procedures. But often that’s not so practical, so you have to have separate processes. They may run on the same machine, but once you have separate processes you immediately have to pay most of the costs in remote calls. Fortunately, SQL is designed as a remote interface, so you can usually arrange things to minimize that cost.</li>
                        <li>Another separation in process may occur in a Web system between the Web server and the application server. All things being equal it’s best to run the Web and application servers in a single process, but all thingsaren’t always equal.</li>
                        <li>You may have to separate because of vendor differences. If you’re using a software package, it will often run in its own process, so again you’re distributing. At least a good package will have a coarse-grained interface.</li>
                        <li>And finally there may be some genuine reason that you have to split your application server software. You should sell any grandparent you can get your hands on to avoid this, but cases do come up. Then you just have to hold your nose and divide your software into remote, coarse-grained components.</li>
                </ul> The overriding theme, in Colleen Roe’s memorable phrase, is to be “parsimonious with object distribution.” Sell your favorite grandma first if you possibly can.</p>
        <h3 id="Working_with_the_Distribution_Boundary">Working with the Distribution Boundary</h3>
        <p>As you design your system you need to limit your distribution boundaries as much as possible, but where you have them you need to take them into account. Every remote call travels on the cyber equivalent of a horse and carriage. All sorts of places in the system will change shape to minimize remote calls. That’s pretty much the expected price. However, you can still design within a single process using fine-grained objects. The key is to use them internally and place coarse-grained objects at the distribution boundaries, whose sole role is to provide a remote interface to the fine-grained objects. The coarse-grained objects don’t really do anything but delegate so they act as a facade for the fine-grained objects. This facade is there only for distribution purposes—hence the name Remote Facade (388). Using a Remote Facade (388) helps minimize the difficulties that the coarsegrained interface introduces. This way only the objects that really need a remote service get the coarse-grained method, and it’s obvious to the developers that they’re paying that cost. Transparency has its virtues, but you don’t want to be transparent about a potential remote call.</p>
        <p>By keeping the coarse-grained interfaces as mere facades, however, you allow people to use the fine-grained objects whenever they know they are running in the same process. This makes the whole distribution policy much more explicit. Hand in hand with Remote Facade (388) is Data Transfer Object (401). Notonly do you need coarse-grained methods, you also need to transfer coarsegrained objects. When you ask for an address, you need to send that information in one block. You usually can’t send the domain object itself, because it’s tied in a Web of fine-grained local inter-object references. So you take all the data that the client needs and bundle it in a particular object for the transfer— hence the term Data Transfer Object (401). (Many people in the enterprise Java community use the term value object for this, but this causes a clash with other meanings of the term Value Object (486)). The Data Transfer Object (401) appears on both sides of the wire, so it’s important that it not reference anything that isn’t shared over the wire. This boils down to the fact that a Data Transfer Object (401) usually only references other Data Transfer Objects (401) and fundamental objects such as strings.</p>
        <p>Another route to distribution is to have a broker that migrates objects between processes. The idea here is to use a Lazy Load (200) scheme where, instead of lazy reading from a database, you move objects across the wire. The hard part of this is ensuring that you don’t end up with lots of remote calls. I haven’t seen anyone try this in an application, but some O/R mapping tools (e.g., TOPLink) have this facility, and I’ve heard some good reports about it.</p>
        <h3 id="Interfaces_for_Distribution">Interfaces for Distribution</h3>
        <p>Traditionally the interfaces for distributed components have been based on remote procedure calls, either with global procedures or as methods on objects. In the last couple of years, however, we’ve begun to see interfaces based on XML over HTTP. SOAP is probably going to be the most common form of this interface, but many people have experimented with it for some years.\ XML-based HTTP communication is handy for several reasons. It easily allows a lot of data to be sent, in structured form, in a single roundtrip. Since remote calls need to be minimized, that’s a good thing. The fact that XML is a common format with parsers available in many platforms allows systems built on very different platforms to communicate, as does the fact that HTTP is pretty universal these days. The fact that XML is textual makes it easy to see what’s going across the wire. HTTP is also easy to get through firewalls when security and political reasons often make it difficult to open up other ports. Even so, an object-oriented interface of classes and methods has value too.</p>
        <p>Moving all the transferred data into XML structures and strings can add a considerable burden to the remote call. Certainly applications have seen a significant performance improvement by replacing an XML-based interface with a remote call. If both sides of the wire use the same binary mechanism, an XML interface doesn’t buy you much other than a jazzier set of acronyms. If you have two systems built with the same platform, then you’re better off using the remote call mechanism built into that platform. Web services become handy when you want different platforms to talk to each other. My attitude is to use XML Web services only when a more direct approach isn’t possible. Of course, you can have the best of both worlds by layering an HTTP interface over an object-oriented interface. All calls to the Web server are translated by it into calls on an underlying object-oriented interface. To an extent this gives you the best of both worlds, but it does add complexity since you’ll need both the Web server and the machinery for a remote OO interface. Therefore, you should only do this if you need an HTTP as well as a remote OO API or if the facilities of the remote OO API for security and transaction handling make it easier to deal with these issues than using non-remote objects.</p>
        <p>In my discussions here I’ve assumed a synchronous, RPC-based interface. However, although that’s what I’ve described, I actually don’t think it’s always the best way of handling a distributed system. Increasingly, my preference is for a message-based approach that’s inherently asynchronous. Digging into patterns for message-based work is a sizable topic on its own, and that’s why I ducked out of it for this book. I hope such a book will appear in the near future, but for the moment all I can do is urge you to look at asynchronous, message-based approaches. In particular I think they’re the best use of Web services, even though most of the examples published so far are synchronous. <i>This page intentionally left blank</i></p>
        <h3 id="Putting_It_All_Together">Chapter 8</h3>
        <h2>Putting It All Together</h2>
        <p>So far these narratives have looked at one aspect of a system and explored the various options for handling it. Now it’s time to sweep everything together and start to answer the tricky question of what patterns to use when designing an enterprise application. The advice in this chapter is in many ways a repeat of the advice given in earlier chapters. I must admit that I’ve wondered whether this chapter was needed. However, I felt it was good to put all the discussion in context now that, I hope, you have at least an outline of the full scope of the patterns in this book. As I write this, I’m very conscious of the limitations of my advice. Frodo said in Lord of the Rings, “Go not to the Elves for counsel, for they will say both no and yes.” While I’m not claiming any immortal knowledge, I certainly understand their answer that advice is often a dangerous gift. If you’re reading this to make architectural decisions for your project, you know far more about your project than I do. One of the biggest frustrations in being a pundit is that people often come up to me at a conference or send an e-mail message asking for advice on their architectural or process decisions. There’s no way you can give particular advice on the basis of a five-minute description. I write this chapter with even less knowledge of your predicament.</p>
        <p>So, read this chapter in the spirit in which it’s presented. I don’t know all the answers, and I certainly don’t know your questions. Use this advice to prod your thinking, but don’t use it as a replacement for your thinking. In the end you have to make, and live with, the decisions yourself. One good thing is that your decisions are not cast forever in stone. Architectural refactoring is hard, and we’re still ignorant of its full costs, but it isn’t impossible. Here the best advice I can give is that, even if you dislike the full story of extreme programming [Beck XP], you should still consider seriously three technical practices: continuous integration [Fowler CI], test driven development [Beck TDD], and refactoring [Fowler Refactoring]. These won’t be a panacea, but they’ll make it much easier for you to change your mind when you discover you need to. And you will need to, unless you’re either more fortunate, or more skillful, than anyone I’ve met to date.</p>
        <h3 id="Starting_with_the_Domain_Layer">Starting with the Domain Layer</h3>
        <p>The start of the process is deciding which domain logic approach to go with. The three main contenders are <i>Transaction Script</i> (110), <i>Table Module</i> (125), and <i>Domain Model</i> (116).</p>
        <p>As I indicated in Chapter 2 (page 25), the strongest force that drives you through this trio is the complexity of the domain logic, something currently impossible to quantify, or even qualify, with any degree of precision. But other factors also play in the decision, in particular, the difficulty of the connection with a database. The simplest of the three patterns is Transaction Script (110). It fits with the procedural model that most people are still comfortable with. It nicely encapsulates the logic of each system transaction in a comprehensible script. And it’s easy to build on top of a relational database. Its great failing is that it doesn’t deal well with complex business logic, being particularly susceptible to duplicate code. If you have a simple catalog application with little more than a shopping cart running off a basic pricing structure, then Transaction Script (110) will fill the bill perfectly. However, as your logic gets more complicated your difficulties multiply exponentially.</p>
        <p>At the other end of the scale is the Domain Model (116). Hard-core object bigots like myself will have an application no other way. After all, if an application is simple enough to write with Transaction Scripts (110), why should our immense intellects bother with such an unworthy problem? Also, my experiences lead me to have no doubt that with really complex domain logic nothing can handle this hell better than a rich Domain Model (116). Once you get used to working with a Domain Model (116) even simple problems can be tackled with ease. Yet the Domain Model (116) has its faults. High on the list is the difficulty of learning how to use a domain model. Object bigots often look down their noses at people who just don’t get objects, but the consequence is that a Domain Model (116) requires skill if it’s to be done well—done poorly it’s a disaster. The second big difficulty of a Domain Model (116) is its connection to a relational database. Of course, a real object zealot finesses this problem with the subtle flick of an object database. But for many, mostly nontechnical, reasons an object database isn’t a possible choice for enterprise applications. The result is the messy relational database connection. Let’s face it, object models and relational models don’t fit well together. The complexity of many of the O/R mapping patterns I describe is the result.</p>
        <p><i>Table Module</i> (125) represents an attractive middle ground between these poles. It can handle domain logic better than Transaction Scripts (110). Also, while it can’t touch a real Domain Model (116) on handling complex domain logic, it fits really well with a relational database—and many other things too. If you have an environment such as .NET, where many tools orbit around the all-seeing Record Set (508), then Table Module (125) works nicely by playing to the strengths of the relational database and yet representing a reasonable factoring of the domain logic. In this argument we see that the tools you have also affect your architecture. Sometimes you’re able to choose the tools based on the architecture, and in theory that’s the way you should go. In practice, however, you often have to match your architecture to your tools. Of the three patterns Table Module (125) is the one whose star rises the most when you have tools that match it. It’s a particularly strong choice for .NET environments, since so much of the platform is geared around Record Set (508). If you read the discussion of domain logic in Chapter 2, much of this will seem familiar. Yet I think it’s worth repeating here because I really do think this is the central decision. From here we go downward to the database layer, but now the decisions are shaped by the context of your domain layer choice.</p>
        <h3 id="Down_to_the_Data_Source_Layer">Down to the Data Source Layer</h3>
        <p>Once you’ve chosen your domain layer, you have to figure out how to connect it to your data sources. Your decisions are based on your domain layer choice, so I’ll tackle this in separate sections, driven by that choice.
        <h4 id="Data_Source_for_Transaction_Script"><i>Data Source for Transaction Script (110)</i></h4>
        <p>The simplest Transaction Scripts (110) contain their own database logic, but I avoid that even in the simplest cases. Separating the database delimits two parts that make sense as separate, so I make the separation even in the simplest applications. The database patterns to choose from here are Row Data Gateway (152) and Table Data Gateway (144). The choice between the two depends much on the facilities of your implementation platform and on where you expect the application to go in the future. With a Row Data Gateway (152) each record is read into an object with a clear and explicit interface. With Table Data Gateway (144) you may have less code to write since you don’t need all the accessor code to get at the data, but you end up with a much more implicit interface that relies on accessing a record set structure that’s little more than a glorified map. The key decision, however, lies in the rest of your platform. Having a platform that provides a lot of tools that work well with Record Set (508), particularly UI tools or transactional disconnected record sets, tilts you decisively in the direction of a Table Data Gateway (144).</p>
        <p>You usually don’t need any of the other O/R mapping patterns in this context. The structural mapping issues are pretty much absent since the in-memory structure maps to the database structure so well. You might consider a Unit of Work (184), but usually it’s easy to keep track of what’s changed in the script. You don’t need to worry about most concurrency issues because the script often corresponds almost exactly to a system transaction. Thus, you can just wrap the whole script in a single transaction. The common exception is where one request pulls data back for editing and the next request tries to save the changes. In this case Optimistic Offline Lock (416) is almost always the best choice. Not only is it easier to implement, it also usually fits users’ expectations and avoids the problem of a hanging session leaving all sorts of things locked.</p>
        <h4 id="Data_Source_Table_Module"><i>Data Source Table Module (125)</i></h4>
        <p>The main reason to choose Table Module (125) is the presence of a good Record Set (508) framework. In this case you’ll want a database mapping pattern that works well with Record Sets (508), and that leads you inexorably toward Table Data Gateway (144). These two patterns fit together as if it were a match made in heaven. There’s not really anything else you need to add on the data source side with this pattern. In the best cases the Record Set (508) has some kind of concurrency control mechanism built in, which effectively turns it into a Unit of Work (184), further reducing hair loss.</p>
        <h4 id="Data_Source_for_Domain_Model"><i>Data Source for Domain Model (116)</i></h4>
        <p>Now things get interesting. In many ways the big weakness of Domain Model (116) is that the connection to the database is complicated. The degree of complication depends on the complexity of this pattern. If your Domain Model (116) is fairly simple, say a couple of dozen classes that are pretty close to the database, then an Active Record (160) makes sense. If you want to decouple things a bit, you can use either Table Data Gateway (144) or Row Data Gateway (152) to do that. Whether you separate or not isn’t a huge deal either way. As things get more complicated, you’ll need to consider Data Mapper (165). This is the approach that delivers on the promise of keeping your Domain Model (116) as independent as possible of all the other layers. But Data Mapper (165) is also the most complicated one to implement. Unless you either have a strong team or you can find some simplifications that make the mapping easier to do, I’d strongly suggest getting a mapping tool. Once you choose Data Mapper (165) most of the patterns in the O/R mapping section come into play. In particular I heartily recommend Unit of Work (184), which acts as a focal point for concurrency control.</p>
        <h4 id="The_Presentation_Layer">The Presentation Layer</h4>
        <p>In many ways the presentation is relatively independent of the choice of the lower layers. Your first question is whether to provide a rich-client interface or an HTML browser interface. A rich client will give you a nicer UI, but then you need a certain amount of control and deployment of your clients. My preference is to pick an HTML browser if you can get away with it and a rich client if that’s not possible. Rich clients will usually take more effort to program, but that’s because they tend to be more sophisticated, not so much because of the inherent complexities of the technology. I haven’t explored any rich-client patterns in this book, so if you choose one I don’t really have anything further to say. If you go the HTML route, you have to decide how to structure your application. I certainly recommend the Model View Controller (330) as the underpinning for your design. That done, you’re left with two decisions, one for the controller and one for the view.</p>
        <p>Your tooling may well make your choice for you. If you use Visual Studio, the easiest way to go is Page Controller (333) and Template View (350). If you use Java, you have a choice of Web frameworks to consider. Popular at the moment is Struts, which will lead you to a Front Controller (344) and a Template View (350). Given a freer choice, I’d recommend Page Controller (333) if your site is more document oriented, particularly if you have a mix of static and dynamic pages. More complex navigation and UI lead you toward a Front Controller (344). On the view front the choice between Template View (350) and Transform View (361) depends on whether your team uses server pages or XSLT in programming. Template Views (350) have the edge at the moment, although I rather like the added testability of Transform View (361). If you have the needto display a common site with multiple looks and feels, you should consider Two Step View (365). How you communicate with the lower layers depends on what kind of layers they are and whether they’re always going to be in the same process. My preference is to have everything run in one process if you can—that way you don’t have to worry about slow inter-process calls. If you can’t do that, you should wrap your domain layer with Remote Facade (388) and use Data Transfer Object (401) to communicate to the Web server. </p>
        <h3 id="Some_Technology_Specific_Advice">Some Technology-Specific Advice</h3>
        <p>In most of this book I’m trying to bring out the common experience of doing projects on many different platforms. Experience with Forte, CORBA, and Smalltalk translates very effectively into developing with Java and .NET. The only reason I’ve concentrating on Java and .NET environments is that they look like the most common platforms for enterprise application development in the future. (Although I’d like to see the dynamically typed scripting languages, in particular Python and Ruby, give them a run for their money.) In this section I want to apply the above advice to these two platforms. As soon as I do this, though, I’m in danger of dating myself. Technologies change much more rapidly than these patterns, so as you read remember that I’m writing in early 2002, when everyone is saying that economic recovery is just around the corner.</p>
        <h3 id="Java_and_J2EE">Java and J2EE</h3>
        <p>Currently the big debate in the Java world is exactly how valuable Enterprise Java Beans are. After as many final drafts as The Who had farewell concerts, the EJB 2.0 specification has finally appeared. But you don’t need EJB to build agood J2EE application, despite what EJB vendors tell you. You can do a great deal with POJOs (plain old Java objects) and JDBC. The design alternatives for J2EE vary in terms of the patterns you’re using, and again they break out by domain logic. If you use Transaction Script (110) on top of some form of Gateway (466), the common approach with EJB at the moment is to use session beans as a Transaction Script (110) and entity beans as a Row Data Gateway (152). This is a pretty reasonable architecture if your domain logic is sufficiently modest. However, one problem with such a beany approach is that it’s hard to get rid of the EJB server if you find you don’t need it and you don’t want to cough up the license fees. The non-EJB approach is a POJO for the Transaction Script (110) on top of either a Row Data Gateway (152) or a Table Data Gateway (144). If JDBC 2.0 row sets get more acceptance, that’s a reason to use them as Record Sets (508) and that leads to a Table Data Gateway (144).</p>
        <p>If you’re using a Domain Model (116), the current orthodoxy is to use entity beans. If your Domain Model (116) is pretty simple and matches your database well, doing that makes reasonable sense and your entity beans will then beActive Records (160). It’s still good practice to wrap your entity beans with session beans acting as Remote Facades (388) (although you can also think of Container Managed Persistance (CMP) as a Data Mapper (165)). However, if your Domain Model (116) is more complex, you want it to be entirely independent of the EJB structure so that you can write, run, and test your domain logic without having to deal with the vagaries of the EJB container. In that model I would use POJOs for the Domain Model (116) and wrap them with session beans acting as Remote Facades (388). If you choose not to use EJB, I would run the whole app in the Web server and avoid any remote calls between presentation and domain. If you’re using POJO Domain Model (116), I would also use POJOs for the Data Mappers (165)—either using an O/R mapping tool or rolling something myself if I felt up to it.</p>
        <p>If you use entity beans in any context, avoid giving them a remote interface. I never understood the point of giving entity beans a remote interface in the first place. Entity beans are usually used as Domain Models (116) or as Row Data Gateways (152). In either case they need a fine-grained interface to play those roles well. As I hope I’ve drilled into your psyche, however, that a remote interface must always be coarse-grained, so keep your entity beans local only. (The exception to this is the Composite Entity pattern from [Alur et al.], which is a different way of using entity beans and not one I find very useful.) At the moment the Table Module (125) isn’t common in the Java world. It will be interesting to see if more tooling surrounds the JDBC row set—if it does this pattern could become a viable approach. In this case the POJO approach fits best, although you can also wrap the Table Module (125) with session beans acting as Remote Facades (388) and returning Record Sets (508).</p>
        <h3 id=".NET">.NET</h3>
        <p>Looking at .NET, Visual Studio, and the history of application development in the Microsoft world, the dominant pattern is Table Module (125). Although object bigots tend to dismiss this as meaning only that Microsofties don’t get objects, Table Module (125) does present a valuable compromise between Transaction Script (110) and Domain Model (116), with an impressive set of tools that take advantage of the ubiquitous data set acting as a Record Set (508). As a result Table Module (125) has to be the default choice for this platform. Indeed, I see no point at all in using Transaction Scripts (110) except in the very simplest of cases, and even then they should act on and return data sets. This doesn’t mean that you can’t use Domain Model (116). Indeed, you can build a Domain Model (116) just as easily in .NET as you can in any other OO environment. However, the tools don’t give you the extra help they do for Table Modules (125), so I would tolerate more complexity before I felt the need to shift to a Domain Model (116). The current hype in .NET is all about Web services, but I wouldn’t use Web services inside an application, I’d use them, as in Java, as a presentation to allow applications to integrate. There’s no real reason to make the Web server and the domain logic into separate processes in a .NET application, so Remote Facade (388) is less useful here. </p>
        <h4 id="Stored_Procedures">Stored Procedures</h4>
        <p>There’s usually a fair bit of debate over stored procedures. They’re often the fastest way to do things since they run in the same process as your database and thus reduce the laggardly remote calls. However, most stored procedure environments don’t give you good structuring mechanisms for your stored procedures, and stored procedures will lock you into a particular database vendor. (Anice way to avoid these problems is Oracle’s approach of allowing you to run Java applications inside your database process; this is equivalent to putting your whole domain logic layer inside the database. For the moment this still leaves you with some vendor lockin, but it at least reduces porting costs.) For the reasons of modularity and portability a lot of people avoid using stored procedures for business logic. I tend to side with that view unless there’s a strong performance gain to be had, which, to be fair, there often is. In that case I take a method from the domain layer and happily move it into a stored procedure. I do this only on clear performance problem areas, treating it as an optimization step rather than as an architectural principle. ([Nilsson] presents a good argument for using stored procedures more widely.) A common way of using stored procedures is to control access to a database, along the lines of a Table Data Gateway (144). I don’t have any strong feelings about whether or not to do this, and from what I’ve seen there’s no strong reasons either way. In any case I prefer to isolate the database access with the same patterns, whether database access is through stored procedures or more regular SQL.</p>
        <h4 id="Web_Services">Web Services</h4>
        <p>As I write this, the general consensus among pundits is that Web services will make reuse a reality and drive system integrators out of business, but I’m not holding my breath. Within these patterns Web services don’t play a huge role because they’re about application integration rather than application construction. You shouldn’t try to break up a single application into Web services that talk to each other unless you really need to. Rather, build your application and expose various parts of it as Web services, treating those Web services as Remote Facades (388). Above all, don’t let all the buzz about how easy it is to build Web services make you forget about the First Law of Distributed Object Design (page 89) . Although most published examples I’ve seen use Web services synchronously, rather like an XML RPC call, I prefer them as asynchronous and message based. While I don’t have any patterns for that here (this book is big enough as it is), I expect that we’ll see some patterns for asynchronous messaging in the next few years.</p>
        <h3 id="Other_Layering_Schemes">Other Layering Schemes</h3>
        <p>I’ve built my discussion around three primary layers, but my approach to layering isn’t the only one that makes sense. Other good architectural books have layering schemes, and they all have value. It’s worth looking at these other schemes and comparing them to what I have here. You may find they make more sense for your application.</p> First up is what I’ll call the Brown model, which is discussed in [Brown et al.] (see Table 8.1). This model has five layers: presentation, controller/mediator, domain, data mapping, and data source. Essentially it places additional mediating layers between the basic three layers. The controller/mediator mediates between the presentation and domain layers, while the data mapping layer mediates between the domain and data source layers. I find that the mediating layers are useful some of the time but not all of the time, so I describe them in terms of patterns. The Application Controller (379) is the mediator between the presentation and domain, and the Data Mapper (165) is the mediator between the data source and the domain. For organizing this book, I’ve described Application Controller (379) in the presentation section (Chapter 14) and Data Mapper (165) in the data source section (Chapter 10). For me, then, the addition of mediating layers, frequently but not always useful, represents an optional extra in the design. My approach is to always think of the three base layers, see if any of them is getting too complex, and if so add the mediating layer to separate the functionality.</p>
        <p>Another good layering scheme for J2EE appears in CoreJ2EE patterns [Alur et al.] (see Table 8.2). Here the layers are client, presentation, business, integration, and resource. Simple correspondences exist for the business and integration layers. The resource layer comprises external services that the integration layer connects to. The main difference is that they split the presentation layer between the part that runs on the client (client) and the part that runs on a server (presentation). This is often a useful split, but again it’s not one that’s needed all the time.</p>
        <p>The Microsoft DNA architect [Kirtland] defines three layers: presentation, business, and data access, that correspond pretty directly to the three layers I use here (see Table 8.3). The biggest shift occurs in the way that data is passed up from the data access layers. In Microsoft DNA all the layers operate on record sets that result from SQL queries issued by the data access layer. This introduces an apparent coupling in that both the business and the presentation layers know about the database.</p>
        <p>The way I look at this is that in DNA the record set acts as a Data Transfer Object (401) between layers. The business layer can modify the record set on its way up to the presentation or even create one itself (that is rare). Although this form of communication is in many ways unwieldy, it has the big advantage of allowing the presentation to use data-aware GUI controls, even on data that’s been modified by the business layer. In this case the domain layer is structured in the form of Table Modules (125) and the data source layer uses Table Data Gateways (144). [Marinescu] has five layers (see Table 8.4). The presentation is split into two layers, reflecting the separation of an Application Controller (379). The domain is also split, with a Service Layer (133) built on a Domain Model (116), reflecting the common idea of splitting a domain layer into two parts. This is a common approach, reinforced by the limitations of EJB as a Domain Model (116) (see page 118). The idea of splitting a services layer from a domain layer is based on a separation of workflow logic from pure domain logic. The services layer typically includes logic that’s particular to a single use case and also some communication with other infrastructures, such as messaging. Whether to have separate services and domain layers is a matter some debate. I tend to look at it as occasionally useful rather than mandatory, but designers I respect disagree with me on this.</p>
        <p>[Nilsson] uses one of the more complex layering schemes (see Table 8.5). Mapping to this scheme is made a bit more complex by the fact that Nilsson uses stored procedures extensively, and encourages domain logic in them for performance reasons. I’m uncomfortable with putting domain logic in stored procedures, as it can make an application much harder to maintain. On occasion, however, it’s a valuable optimization technique. Nilsson’s stored procedure layers contain both data source and domain logic. Like [Marinescu], Nilsson uses separate application and domain layers for domain logic. He suggests that you can skip the domain layer in a small system, which is similar to my view that a Domain Model (116) is less worthwhile for smaller systems.</p>
</body>
</html>